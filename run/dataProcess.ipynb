{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(r\"../\")\n",
    "\n",
    "dataDir='../Data/Causal/'\n",
    "storage_path='../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import Syn_Generator, IHDP_Generator, Twins_Generator\n",
    "\n",
    "Syn_244 = Syn_Generator(n=10000, ate=0,sc=1,sh=0,one=1,depX=0.05,depU=0.05,VX=1,mV=2,mX=4,mU=4,init_seed=7,seed_coef=10,details=1,storage_path=storage_path)\n",
    "Syn_244.run(n=10000, num_reps=10)\n",
    "\n",
    "IHDP_242 = IHDP_Generator(mV=2, mX=4, mU=2,details=1,dataDir=dataDir, storage_path=storage_path)\n",
    "IHDP_242.run(100)\n",
    "\n",
    "IHDP_260 = IHDP_Generator(mV=2, mX=6, mU=0,details=1,dataDir=dataDir, storage_path=storage_path)\n",
    "IHDP_260.run(100)\n",
    "\n",
    "Twins_553 = Twins_Generator(sc=1, sh=-2, mV=5, mX=5, mU=3, details=1,dataDir=dataDir, storage_path=storage_path)\n",
    "Twins_553.run(10)\n",
    "\n",
    "Twins_580 = Twins_Generator(sc=1, sh=-2, mV=5, mX=8, mU=0, details=1,dataDir=dataDir, storage_path=storage_path)\n",
    "Twins_580.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets = [Syn_244, IHDP_242, IHDP_260, Twins_553, Twins_580]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain: Stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import log, CausalDataset\n",
    "from module.Regression import run as run_Reg\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def get_args():\n",
    "    argparser = argparse.ArgumentParser(description=__doc__)\n",
    "    # About run setting !!!!\n",
    "    argparser.add_argument('--seed',default=2021,type=int,help='The random seed')\n",
    "    argparser.add_argument('--mode',default='vx',type=str,help='The choice of v/x/vx/xx')\n",
    "    argparser.add_argument('--rewrite_log',default=False,type=bool,help='Whether rewrite log file')\n",
    "    argparser.add_argument('--use_gpu',default=True,type=bool,help='The use of GPU')\n",
    "    # About data setting ~~~~\n",
    "    argparser.add_argument('--num',default=10000,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--num_reps',default=10,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--ate',default=0,type=float,help='The ate of constant')\n",
    "    argparser.add_argument('--mV',default=2,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mX',default=4,type=int,help='The dim of Confounding variables X')\n",
    "    argparser.add_argument('--mU',default=4,type=int,help='The dim of Unobserved confounding variables U')\n",
    "    argparser.add_argument('--storage_path',default='../Data/',type=str,help='The dir of data storage')\n",
    "    # About Regression_t\n",
    "    argparser.add_argument('--regt_batch_size',default=500,type=int,help='The size of one batch')\n",
    "    argparser.add_argument('--regt_lr',default=0.05,type=float,help='The learning rate')\n",
    "    argparser.add_argument('--regt_num_epoch',default=3,type=int,help='The num of total epoch')\n",
    "    args = argparser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run Syn_244_xx\n",
    "for mode in ['xx','v','x']:\n",
    "    data = Datasets[0]\n",
    "    which_benchmark = data.which_benchmark\n",
    "    which_dataset = data.which_dataset\n",
    "    args.num_reps = data.num_reps\n",
    "    args.mV = data.mV\n",
    "    args.mX = data.mX\n",
    "    args.mU = data.mU\n",
    "    args.mode = mode\n",
    "\n",
    "    resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}/'\n",
    "    dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "    os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "\n",
    "    if args.rewrite_log:\n",
    "        f = open(logfile,'w')\n",
    "        f.close()\n",
    "\n",
    "    for exp in range(args.num_reps):\n",
    "        train_df = pd.read_csv(dataDir + f'{exp}/train.csv')\n",
    "        val_df = pd.read_csv(dataDir + f'{exp}/val.csv')\n",
    "        test_df = pd.read_csv(dataDir + f'{exp}/test.csv')\n",
    "\n",
    "        train = CausalDataset(train_df, variables = ['u','x','v','z','p','s','m','t','g','y','f','c'])\n",
    "        val = CausalDataset(val_df, variables = ['u','x','v','z','p','s','m','t','g','y','f','c'])\n",
    "        test = CausalDataset(test_df, variables = ['u','x','v','z','p','s','m','t','g','y','f','c'])\n",
    "\n",
    "        train,val,test = run_Reg(exp, args, dataDir, resultDir, train, val, test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run vx\n",
    "for data in Datasets:\n",
    "    which_benchmark = data.which_benchmark\n",
    "    which_dataset = data.which_dataset\n",
    "    args.num_reps = data.num_reps\n",
    "    args.mV = data.mV\n",
    "    args.mX = data.mX\n",
    "    args.mU = data.mU\n",
    "    args.mode = 'vx'\n",
    "\n",
    "    resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}/'\n",
    "    dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "    os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "\n",
    "    if args.rewrite_log:\n",
    "        f = open(logfile,'w')\n",
    "        f.close()\n",
    "\n",
    "    for exp in range(args.num_reps):\n",
    "        train_df = pd.read_csv(dataDir + f'{exp}/train.csv')\n",
    "        val_df = pd.read_csv(dataDir + f'{exp}/val.csv')\n",
    "        test_df = pd.read_csv(dataDir + f'{exp}/test.csv')\n",
    "\n",
    "        train = CausalDataset(train_df, variables = ['u','x','v','z','p','s','m','t','g','y','f','c'])\n",
    "        val = CausalDataset(val_df, variables = ['u','x','v','z','p','s','m','t','g','y','f','c'])\n",
    "        test = CausalDataset(test_df, variables = ['u','x','v','z','p','s','m','t','g','y','f','c'])\n",
    "\n",
    "        train,val,test = run_Reg(exp, args, dataDir, resultDir, train, val, test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.7.1",
   "language": "python",
   "name": "pytorch-1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
