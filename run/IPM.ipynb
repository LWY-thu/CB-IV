{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(r\"../\")\n",
    "sys.path.append(r\"../../\")\n",
    "sys.path.append('/home/wyliu/code/CB-IV')\n",
    "from utils.imbFun import *\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import log, CausalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    argparser = argparse.ArgumentParser(description=__doc__)\n",
    "    # About run setting !!!!\n",
    "    argparser.add_argument('--seed',default=2021,type=int,help='The random seed')\n",
    "    argparser.add_argument('--mode',default='vx',type=str,help='The choice of v/x/vx/xx')\n",
    "    argparser.add_argument('--rewrite_log',default=False,type=bool,help='Whether rewrite log file')\n",
    "    argparser.add_argument('--use_gpu',default=False,type=bool,help='The use of GPU')\n",
    "    argparser.add_argument('--ood',default=0,type=float,help='The train dataset of OOD')\n",
    "    # About data setting ~~~~\n",
    "    argparser.add_argument('--num',default=10000,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--num_reps',default=10,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--ate',default=0,type=float,help='The ate of constant')\n",
    "    argparser.add_argument('--sc',default=1,type=float,help='The sc')\n",
    "    argparser.add_argument('--sh',default=0,type=float,help='The sh')\n",
    "    argparser.add_argument('--one',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--depX',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--depU',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--VX',default=0,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mV',default=2,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mX',default=10,type=int,help='The dim of Confounding variables X')\n",
    "    argparser.add_argument('--mU',default=4,type=int,help='The dim of Unobserved confounding variables U')\n",
    "    argparser.add_argument('--mXs',default=2,type=int,help='The dim of Noise variables X')\n",
    "    \n",
    "    argparser.add_argument('--storage_path',default='../../Data/',type=str,help='The dir of data storage')\n",
    "    # Syn\n",
    "    argparser.add_argument('--syn_alpha',default=0.01,type=float,help='')\n",
    "    argparser.add_argument('--syn_lambda',default=0.0001,type=float,help='')\n",
    "    argparser.add_argument('--syn_twoStage',default=True,type=bool,help='')\n",
    "    # About Debug or Show\n",
    "    argparser.add_argument('--verbose',default=1,type=int,help='The level of verbose')\n",
    "    argparser.add_argument('--epoch_show',default=5,type=int,help='The epochs of show time')\n",
    "    args = argparser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "def get_FLAGS():\n",
    "    ''' Define parameter flags '''\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "    tf.app.flags.DEFINE_integer('lrate_decay_num', 100, \"\"\"NUM_ITERATIONS_PER_DECAY. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('seed', 2021, \"\"\"Seed. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('debug', 0, \"\"\"Debug mode. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('save_rep', 0, \"\"\"Save representations after training. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('output_csv',0,\"\"\"Whether to save a CSV file with the results\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('output_delay', 100, \"\"\"Number of iterations between log/loss outputs. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('x_key', 'x', \"\"\"Which key to use (x/xu/vxu)\"\"\")\n",
    "    tf.app.flags.DEFINE_string('loss', 'l2', \"\"\"Which loss function to use (l1/l2/log)\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('n_in', 3, \"\"\"Number of representation layers. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('n_out', 5, \"\"\"Number of regression layers. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('p_alpha', 1, \"\"\"Imbalance regularization param. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('p_lambda', 1e-4, \"\"\"Weight decay regularization parameter. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('rep_weight_decay', 0, \"\"\"Whether to penalize representation layers with weight decay\"\"\")\n",
    "    tf.app.flags.DEFINE_float('dropout_in', 1.0, \"\"\"Input layers dropout keep rate. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('dropout_out', 1.0, \"\"\"Output layers dropout keep rate. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('nonlin', 'elu', \"\"\"Kind of non-linearity. Default relu. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('lrate', 5e-4, \"\"\"Learning rate. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('decay', 0.3, \"\"\"RMSProp decay. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('batch_size', 256, \"\"\"Batch size. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('dim_in', 256, \"\"\"Pre-representation layer dimensions. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('dim_out', 256, \"\"\"Post-representation layer dimensions. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('batch_norm', 0, \"\"\"Whether to use batch normalization. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('normalization', 'none', \"\"\"How to normalize representation (after batch norm). none/bn_fixed/divide/project \"\"\")\n",
    "    tf.app.flags.DEFINE_float('rbf_sigma', 0.1, \"\"\"RBF MMD sigma \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('experiments', 2, \"\"\"Number of experiments. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('iterations', 300, \"\"\"Number of iterations. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('weight_init', 0.1, \"\"\"Weight initialization scale. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('lrate_decay', 0.97, \"\"\"Decay of learning rate every 100 iterations \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('wass_iterations', 10, \"\"\"Number of iterations in Wasserstein computation. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('wass_lambda', 10.0, \"\"\"Wasserstein lambda. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('wass_bpt', 1, \"\"\"Backprop through T matrix? \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('varsel', 0, \"\"\"Whether the first layer performs variable selection. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('outdir', '../Data/DRCFR/results/', \"\"\"Output directory. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('datadir', '../Data/DRCFR/data/Syn_1.0_1.0_0/2_4_4/', \"\"\"Data directory. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('dataform', 'train_0.csv', \"\"\"Training data filename form. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('data_val', 'val_0.csv', \"\"\"Valid data filename form. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('data_test', 'test_0.csv', \"\"\"Test data filename form. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('sparse', 0, \"\"\"Whether data is stored in sparse format (.x, .y). \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('repetitions', 1, \"\"\"Repetitions with different seed.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('use_p_correction', 0, \"\"\"Whether to use population size p(t) in mmd/disc/wass.\"\"\")\n",
    "    tf.app.flags.DEFINE_string('optimizer', 'Adam', \"\"\"Which optimizer to use. (RMSProp/Adagrad/GradientDescent/Adam)\"\"\")\n",
    "    tf.app.flags.DEFINE_string('imb_fun', 'wass', \"\"\"Which imbalance penalty to use (mmd_lin/mmd_rbf/mmd2_lin/mmd2_rbf/lindisc/wass). \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('pred_output_delay', 200, \"\"\"Number of iterations between prediction outputs. (-1 gives no intermediate output). \"\"\")\n",
    "    tf.app.flags.DEFINE_float('val_part', 0.3, \"\"\"Validation part. \"\"\")\n",
    "    tf.app.flags.DEFINE_boolean('split_output', 1, \"\"\"Whether to split output layers between treated and control. \"\"\")\n",
    "    tf.app.flags.DEFINE_boolean('reweight_sample', 1, \"\"\"Whether to reweight sample for prediction loss with average treatment probability. \"\"\")\n",
    "    tf.app.flags.DEFINE_boolean('twoStage', 1, \"\"\"twoStage. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "    tf.app.flags.DEFINE_string('ip', '', 'kernel')\n",
    "    tf.app.flags.DEFINE_integer('mV', 2, \"\"\"The dim of Instrumental variables V.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('mX', 4, \"\"\"The dim of Confounding variables X.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('mU', 4, \"\"\"The dim of Unobserved confounding variables U.\"\"\")\n",
    "    tf.app.flags.DEFINE_float('ood', 0., \"\"\"ood. \"\"\")\n",
    "\n",
    "    if FLAGS.sparse:\n",
    "        import scipy.sparse as sparse\n",
    "\n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = get_FLAGS()\n",
    "# set OOD path\n",
    "''' bias rate '''\n",
    "args.ood = -3.0\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 0.0, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "which_benchmark = 'SynOOD_'+'_'.join(str(item) for item in [args.sc, args.sh, args.one, args.depX, args.depU,args.VX])\n",
    "which_dataset = '_'.join(str(item) for item in [args.mV, args.mX, args.mU, args.mXs])\n",
    "resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}_{args.mode}/ood{brdc[args.ood]}/'\n",
    "dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "logfile = f'{resultDir}/log.txt'\n",
    "''' bias rate '''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 0.0, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd2_rbf_ood(Xc,Xt,p,sig):\n",
    "    \"\"\" Computes the l2-RBF MMD for X given t \"\"\"\n",
    "\n",
    "    # it = tf.where(t>0)[:,0]\n",
    "    # ic = tf.where(t<1)[:,0]\n",
    "\n",
    "    # Xc = tf.gather(X,ic)\n",
    "    # Xt = tf.gather(X,it)\n",
    "    Xc = tf.convert_to_tensor(Xc, dtype=tf.float32)\n",
    "    Xt = tf.convert_to_tensor(Xt, dtype=tf.float32)\n",
    "    Kcc = tf.exp(-pdist2sq(Xc,Xc)/tf.square(sig))\n",
    "    Kct = tf.exp(-pdist2sq(Xc,Xt)/tf.square(sig))\n",
    "    Ktt = tf.exp(-pdist2sq(Xt,Xt)/tf.square(sig))\n",
    "\n",
    "    m = tf.to_float(tf.shape(Xc)[0])\n",
    "    n = tf.to_float(tf.shape(Xt)[0])\n",
    "\n",
    "    mmd = tf.square(1.0-p)/(m*(m-1.0))*(tf.reduce_sum(Kcc)-m)\n",
    "    mmd = mmd + tf.square(p)/(n*(n-1.0))*(tf.reduce_sum(Ktt)-n)\n",
    "    mmd = mmd - 2.0*p*(1.0-p)/(m*n)*tf.reduce_sum(Kct)\n",
    "    mmd = 4.0*mmd\n",
    "\n",
    "    return mmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_ood(Xc,Xt, p,lam=10,its=10,sq=False,backpropT=False):\n",
    "    \"\"\" Returns the Wasserstein distance between treatment groups \"\"\"\n",
    "\n",
    "    # it = tf.where(t1>0)[:,0]\n",
    "    # ic = tf.where(t2>0)[:,0]\n",
    "    # Xc = tf.gather(X1,ic)\n",
    "    # Xt = tf.gather(X2,it)\n",
    "    Xc = tf.convert_to_tensor(Xc, dtype=tf.float32)\n",
    "    Xt = tf.convert_to_tensor(Xt, dtype=tf.float32)\n",
    "    nc = tf.to_float(tf.shape(Xc)[0])\n",
    "    nt = tf.to_float(tf.shape(Xt)[0])\n",
    "\n",
    "    ''' Compute distance matrix'''\n",
    "    if sq:\n",
    "        M = pdist2sq(Xt,Xc)\n",
    "    else:\n",
    "        M = safe_sqrt(pdist2sq(Xt,Xc))\n",
    "\n",
    "    ''' Estimate lambda and delta '''\n",
    "    M_mean = tf.reduce_mean(M)\n",
    "    M_drop = tf.nn.dropout(M,10/(nc*nt))\n",
    "    delta = tf.stop_gradient(tf.reduce_max(M))\n",
    "    eff_lam = tf.stop_gradient(lam/M_mean)\n",
    "\n",
    "    ''' Compute new distance matrix '''\n",
    "    Mt = M\n",
    "    row = delta*tf.ones(tf.shape(M[0:1,:]))\n",
    "    col = tf.concat([delta*tf.ones(tf.shape(M[:,0:1])),tf.zeros((1,1))],0)\n",
    "    Mt = tf.concat([M,row],0)\n",
    "    Mt = tf.concat([Mt,col],1)\n",
    "\n",
    "    ''' Compute marginal vectors '''\n",
    "    # print('nt', nt)\n",
    "    a = tf.concat([p*tf.ones((10000,1))/nt, (1-p)*tf.ones((1,1))],0)\n",
    "    b = tf.concat([(1-p)*tf.ones((10000,1))/nc, p*tf.ones((1,1))],0)\n",
    "\n",
    "    ''' Compute kernel matrix'''\n",
    "    Mlam = eff_lam*Mt\n",
    "    K = tf.exp(-Mlam) + 1e-6 # added constant to avoid nan\n",
    "    U = K*Mt\n",
    "    ainvK = K/a\n",
    "\n",
    "    u = a\n",
    "    for i in range(0,its):\n",
    "        u = 1.0/(tf.matmul(ainvK,(b/tf.transpose(tf.matmul(tf.transpose(u),K)))))\n",
    "    v = b/(tf.transpose(tf.matmul(tf.transpose(u),K)))\n",
    "\n",
    "    T = u*(tf.transpose(v)*K)\n",
    "\n",
    "    if not backpropT:\n",
    "        T = tf.stop_gradient(T)\n",
    "\n",
    "    E = T*Mt\n",
    "    D = 2*tf.reduce_sum(E)\n",
    "\n",
    "    return D, Mlam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 0\n",
    "args.ood = 3.0\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/{args.mode}/ood_{brdc[args.ood]}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/{args.mode}/ood_{brdc[args.ood]}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/{args.mode}/ood_{brdc[args.ood]}/test.csv')\n",
    "train = CausalDataset(train_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "val = CausalDataset(val_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "test = CausalDataset(test_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "\n",
    "x_list = [np.concatenate((train.x, train.xs), 1), \n",
    "            np.concatenate((val.x, val.xs), 1), \n",
    "            np.concatenate((test.x, test.xs), 1)]\n",
    "\n",
    "train = {'x':x_list[0],\n",
    "        't':train.t,\n",
    "        's':train.s,\n",
    "        'g':train.g,\n",
    "        'yf':train.y,\n",
    "        'ycf':train.f}\n",
    "val = {'x':x_list[1],\n",
    "        't':val.t,\n",
    "        's':val.s,\n",
    "        'g':val.g,\n",
    "        'yf':val.y,\n",
    "        'ycf':val.f}\n",
    "test = {'x':x_list[2],\n",
    "        't':test.t,\n",
    "        's':test.s,\n",
    "        'g':test.g,\n",
    "        'yf':test.y,\n",
    "        'ycf':test.f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_n30/train.csv\n",
      "3.781878\n",
      "1\n",
      "2\n",
      "-1.2800798e-06\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_n25/train.csv\n",
      "3.7763948\n",
      "1\n",
      "2\n",
      "-1.4200971e-06\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_n20/train.csv\n",
      "3.7748523\n",
      "1\n",
      "2\n",
      "-1.3400266e-06\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_n15/train.csv\n",
      "3.7511878\n",
      "1\n",
      "2\n",
      "-1.4199364e-06\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_n13/train.csv\n",
      "3.7230735\n",
      "1\n",
      "2\n",
      "-1.3398154e-06\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_0/train.csv\n",
      "3.6469154\n",
      "1\n",
      "2\n",
      "-2.0401626e-06\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_p13/train.csv\n",
      "2.8291416\n",
      "1\n",
      "2\n",
      "-2.9639996e-05\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_p15/train.csv\n",
      "2.695561\n",
      "1\n",
      "2\n",
      "-4.781998e-05\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_p20/train.csv\n",
      "2.5198214\n",
      "1\n",
      "2\n",
      "-7.604012e-05\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_p25/train.csv\n",
      "2.4388702\n",
      "1\n",
      "2\n",
      "-9.0920126e-05\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/0/vx/ood_p30/train.csv\n",
      "1.7705169\n",
      "1\n",
      "2\n",
      "-0.0002\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_n30/train.csv\n",
      "3.7820234\n",
      "1\n",
      "2\n",
      "-7.813281e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_n25/train.csv\n",
      "3.781487\n",
      "1\n",
      "2\n",
      "-8.789942e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_n20/train.csv\n",
      "3.7811453\n",
      "1\n",
      "2\n",
      "-1.8556542e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_n15/train.csv\n",
      "3.754891\n",
      "1\n",
      "2\n",
      "-4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_n13/train.csv\n",
      "3.7269232\n",
      "1\n",
      "2\n",
      "-4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_0/train.csv\n",
      "3.6662712\n",
      "1\n",
      "2\n",
      "-8.789942e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_p13/train.csv\n",
      "2.9565809\n",
      "1\n",
      "2\n",
      "-1.2696583e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_p15/train.csv\n",
      "2.9154205\n",
      "1\n",
      "2\n",
      "1.5626563e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_p20/train.csv\n",
      "2.8811755\n",
      "1\n",
      "2\n",
      "-3.0276465e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_p25/train.csv\n",
      "2.873406\n",
      "1\n",
      "2\n",
      "2.9299806e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/1/vx/ood_p30/train.csv\n",
      "2.8670177\n",
      "1\n",
      "2\n",
      "-4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_n30/train.csv\n",
      "3.7978659\n",
      "1\n",
      "2\n",
      "-1.6603223e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_n25/train.csv\n",
      "3.786747\n",
      "1\n",
      "2\n",
      "-3.5159767e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_n20/train.csv\n",
      "3.774989\n",
      "1\n",
      "2\n",
      "-1.4649902e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_n15/train.csv\n",
      "3.7456396\n",
      "1\n",
      "2\n",
      "-7.813281e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_n13/train.csv\n",
      "3.7281113\n",
      "1\n",
      "2\n",
      "-3.3206446e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_0/train.csv\n",
      "3.6645868\n",
      "1\n",
      "2\n",
      "-6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_p13/train.csv\n",
      "2.952714\n",
      "1\n",
      "2\n",
      "-1.7579883e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_p15/train.csv\n",
      "2.9056659\n",
      "1\n",
      "2\n",
      "1.5626563e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_p20/train.csv\n",
      "2.8774853\n",
      "1\n",
      "2\n",
      "9.7666014e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_p25/train.csv\n",
      "2.8732162\n",
      "1\n",
      "2\n",
      "-7.813281e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/2/vx/ood_p30/train.csv\n",
      "2.8695998\n",
      "1\n",
      "2\n",
      "-1.0743262e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_n30/train.csv\n",
      "3.791418\n",
      "1\n",
      "2\n",
      "-5.859961e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_n25/train.csv\n",
      "3.7958848\n",
      "1\n",
      "2\n",
      "4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_n20/train.csv\n",
      "3.7785287\n",
      "1\n",
      "2\n",
      "-4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_n15/train.csv\n",
      "3.7617798\n",
      "1\n",
      "2\n",
      "6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_n13/train.csv\n",
      "3.7269049\n",
      "1\n",
      "2\n",
      "6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_0/train.csv\n",
      "3.6582024\n",
      "1\n",
      "2\n",
      "1.4649902e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_p13/train.csv\n",
      "2.9587908\n",
      "1\n",
      "2\n",
      "1.9533203e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_p15/train.csv\n",
      "2.912887\n",
      "1\n",
      "2\n",
      "6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_p20/train.csv\n",
      "2.8814485\n",
      "1\n",
      "2\n",
      "-3.9066406e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_p25/train.csv\n",
      "2.872859\n",
      "1\n",
      "2\n",
      "-9.766602e-12\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/3/vx/ood_p30/train.csv\n",
      "2.8675163\n",
      "1\n",
      "2\n",
      "-8.789942e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_n30/train.csv\n",
      "3.7877135\n",
      "1\n",
      "2\n",
      "-1.439646e-20\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_n25/train.csv\n",
      "3.7826743\n",
      "1\n",
      "2\n",
      "-4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_n20/train.csv\n",
      "3.7744298\n",
      "1\n",
      "2\n",
      "-1.0743262e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_n15/train.csv\n",
      "3.7524717\n",
      "1\n",
      "2\n",
      "-2.8323144e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_n13/train.csv\n",
      "3.7195165\n",
      "1\n",
      "2\n",
      "-8.789942e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_0/train.csv\n",
      "3.6625714\n",
      "1\n",
      "2\n",
      "9.766602e-12\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_p13/train.csv\n",
      "2.95579\n",
      "1\n",
      "2\n",
      "-1.9533203e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_p15/train.csv\n",
      "2.9085708\n",
      "1\n",
      "2\n",
      "1.8556542e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_p20/train.csv\n",
      "2.8774128\n",
      "1\n",
      "2\n",
      "-1.0743262e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_p25/train.csv\n",
      "2.870656\n",
      "1\n",
      "2\n",
      "-8.789942e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/4/vx/ood_p30/train.csv\n",
      "2.8684754\n",
      "1\n",
      "2\n",
      "-1.3673242e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_n30/train.csv\n",
      "3.7923906\n",
      "1\n",
      "2\n",
      "-2.1486524e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_n25/train.csv\n",
      "3.778942\n",
      "1\n",
      "2\n",
      "-2.0509863e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_n20/train.csv\n",
      "3.7827413\n",
      "1\n",
      "2\n",
      "-2.2463184e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_n15/train.csv\n",
      "3.7574162\n",
      "1\n",
      "2\n",
      "-1.8556542e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_n13/train.csv\n",
      "3.7270372\n",
      "1\n",
      "2\n",
      "-2.0509863e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_0/train.csv\n",
      "3.6461554\n",
      "1\n",
      "2\n",
      "-2.1486524e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_p13/train.csv\n",
      "2.9578042\n",
      "1\n",
      "2\n",
      "-2.4416505e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_p15/train.csv\n",
      "2.910123\n",
      "1\n",
      "2\n",
      "1.9533203e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_p20/train.csv\n",
      "2.8771598\n",
      "1\n",
      "2\n",
      "6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_p25/train.csv\n",
      "2.8730555\n",
      "1\n",
      "2\n",
      "-8.789942e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/5/vx/ood_p30/train.csv\n",
      "2.8644838\n",
      "1\n",
      "2\n",
      "-1.0743262e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_n30/train.csv\n",
      "3.794104\n",
      "1\n",
      "2\n",
      "1.4649902e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_n25/train.csv\n",
      "3.7780342\n",
      "1\n",
      "2\n",
      "1.1719922e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_n20/train.csv\n",
      "3.7729704\n",
      "1\n",
      "2\n",
      "-3.9066406e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_n15/train.csv\n",
      "3.75028\n",
      "1\n",
      "2\n",
      "2.9299806e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_n13/train.csv\n",
      "3.7299025\n",
      "1\n",
      "2\n",
      "-9.766602e-12\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_0/train.csv\n",
      "3.6590533\n",
      "1\n",
      "2\n",
      "-1.4649902e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_p13/train.csv\n",
      "2.9627032\n",
      "1\n",
      "2\n",
      "-2.2463184e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_p15/train.csv\n",
      "2.9079413\n",
      "1\n",
      "2\n",
      "-2.9299806e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_p20/train.csv\n",
      "2.878038\n",
      "1\n",
      "2\n",
      "-2.2463184e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_p25/train.csv\n",
      "2.8708363\n",
      "1\n",
      "2\n",
      "-2.9299806e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/6/vx/ood_p30/train.csv\n",
      "2.868503\n",
      "1\n",
      "2\n",
      "1.5626563e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_n30/train.csv\n",
      "3.7802477\n",
      "1\n",
      "2\n",
      "-2.4416505e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_n25/train.csv\n",
      "3.7802942\n",
      "1\n",
      "2\n",
      "-2.9299804e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_n20/train.csv\n",
      "3.7706134\n",
      "1\n",
      "2\n",
      "-1.4649902e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_n15/train.csv\n",
      "3.749587\n",
      "1\n",
      "2\n",
      "-4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_n13/train.csv\n",
      "3.712812\n",
      "1\n",
      "2\n",
      "-5.859961e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_0/train.csv\n",
      "3.6573822\n",
      "1\n",
      "2\n",
      "-1.0743262e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_p13/train.csv\n",
      "2.9565368\n",
      "1\n",
      "2\n",
      "1.8556542e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_p15/train.csv\n",
      "2.9106796\n",
      "1\n",
      "2\n",
      "1.9533203e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_p20/train.csv\n",
      "2.8796844\n",
      "1\n",
      "2\n",
      "-1.4649902e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_p25/train.csv\n",
      "2.8747444\n",
      "1\n",
      "2\n",
      "6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/7/vx/ood_p30/train.csv\n",
      "2.8724954\n",
      "1\n",
      "2\n",
      "-1.9533203e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_n30/train.csv\n",
      "3.7867432\n",
      "1\n",
      "2\n",
      "6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_n25/train.csv\n",
      "3.7889278\n",
      "1\n",
      "2\n",
      "6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_n20/train.csv\n",
      "3.776822\n",
      "1\n",
      "2\n",
      "2.9299806e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_n15/train.csv\n",
      "3.7563858\n",
      "1\n",
      "2\n",
      "-1.5626563e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_n13/train.csv\n",
      "3.713745\n",
      "1\n",
      "2\n",
      "-1.5626563e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_0/train.csv\n",
      "3.6640825\n",
      "1\n",
      "2\n",
      "-3.2229786e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_p13/train.csv\n",
      "2.9594543\n",
      "1\n",
      "2\n",
      "-3.2229786e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_p15/train.csv\n",
      "2.905735\n",
      "1\n",
      "2\n",
      "-2.2463184e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_p20/train.csv\n",
      "2.8814406\n",
      "1\n",
      "2\n",
      "-1.6603223e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_p25/train.csv\n",
      "2.86909\n",
      "1\n",
      "2\n",
      "-1.8556542e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/8/vx/ood_p30/train.csv\n",
      "2.8657842\n",
      "1\n",
      "2\n",
      "-2.5393165e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_n30/train.csv\n",
      "3.7828922\n",
      "1\n",
      "2\n",
      "-6.836621e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_n25/train.csv\n",
      "3.7824001\n",
      "1\n",
      "2\n",
      "-2.4416505e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_n20/train.csv\n",
      "3.7704897\n",
      "1\n",
      "2\n",
      "-1.6603223e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_n15/train.csv\n",
      "3.7477746\n",
      "1\n",
      "2\n",
      "-3.3206446e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_n13/train.csv\n",
      "3.7174883\n",
      "1\n",
      "2\n",
      "-2.4416505e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_0/train.csv\n",
      "3.656979\n",
      "1\n",
      "2\n",
      "2.9299806e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_p13/train.csv\n",
      "2.9608448\n",
      "1\n",
      "2\n",
      "-4.8833007e-11\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_p15/train.csv\n",
      "2.9081492\n",
      "1\n",
      "2\n",
      "-1.2696583e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_p20/train.csv\n",
      "2.8818915\n",
      "1\n",
      "2\n",
      "-5.176299e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_p25/train.csv\n",
      "2.8745732\n",
      "1\n",
      "2\n",
      "-2.2463184e-10\n",
      "../../Data//data/SynOOD_1_0_1_0.05_0.05_0/2_10_4_2/9/vx/ood_p30/train.csv\n",
      "2.864541\n",
      "1\n",
      "2\n",
      "-3.1253125e-10\n"
     ]
    }
   ],
   "source": [
    "wass_dist = []\n",
    "mmd_dist = []\n",
    "results_ood = [wass_dist, mmd_dist]\n",
    "name_ood = [\"wass_dist\", \"mmd_dist\"]\n",
    "for exp in range(10): \n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for r in br:    \n",
    "        train_df_ood = pd.read_csv(dataDir + f'{exp}/{args.mode}/ood_{brdc[r]}/train.csv')\n",
    "        # val_df_ood = pd.read_csv(dataDir + f'{exp}/{args.mode}/ood_{brdc[args.ood]}/val.csv')\n",
    "        # test_df_ood = pd.read_csv(dataDir + f'{exp}/{args.mode}/ood_{brdc[args.ood]}/test.csv')\n",
    "        train_ood = CausalDataset(train_df_ood, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        # val_ood = CausalDataset(val_df_ood, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        # test_ood = CausalDataset(test_df_ood, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        print(dataDir + f'{exp}/{args.mode}/ood_{brdc[r]}/train.csv')\n",
    "        x_list = [np.concatenate((train_ood.x, train_ood.xs), 1)]\n",
    "\n",
    "        train_ood = {'x':x_list[0],\n",
    "                't':train_ood.t,\n",
    "                's':train_ood.s,\n",
    "                'g':train_ood.g,\n",
    "                'yf':train_ood.y,\n",
    "                'ycf':train_ood.f}\n",
    "        p_ipm = 0.5\n",
    "        imb_dist, imB_mat = wasserstein_ood(train['x'],train_ood['x'],p_ipm)\n",
    "        # 创建 TensorFlow 会话\n",
    "        with tf.Session() as sess:\n",
    "            # 执行计算图并获取张量的值\n",
    "            tensor_value = sess.run(imb_dist)\n",
    "        print(tensor_value)\n",
    "        l1.append(tensor_value)\n",
    "\n",
    "        imb_dist_mmd = mmd2_rbf_ood(train['x'],train_ood['x'],p_ipm, FLAGS.rbf_sigma)\n",
    "        # 创建 TensorFlow 会话\n",
    "        with tf.Session() as sess:\n",
    "            # 执行计算图并获取张量的值\n",
    "            tensor_value_mmd = sess.run(imb_dist_mmd)\n",
    "        print(tensor_value_mmd)\n",
    "        l2.append(tensor_value_mmd)\n",
    "    wass_dist.append(l1)\n",
    "    mmd_dist.append(l2)\n",
    "\n",
    "\n",
    "for res, name in zip(results_ood, name_ood):\n",
    "    res.append(np.mean(res[:][:args.num_reps],0))\n",
    "    res.append(np.std(res[:][:args.num_reps],0))\n",
    "    res_df = pd.DataFrame(np.array(res), columns=[brdc[r] for r in br ]).round(4)\n",
    "    res_df.to_csv(resultDir + f'IPM_{args.mode}_{brdc[r]}_' + name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../Data//results/SynOOD_1_0_1_0.05_0.05_0_2_10_4_2_vx/oodn30/'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ipm = 0.5\n",
    "imb_dist, imB_mat = wasserstein_ood(train['x'],test['x'],p_ipm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.702166\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 假设有一个名为 tensor 的 TensorFlow 张量\n",
    "tensor_v = imb_dist\n",
    "# 创建 TensorFlow 会话\n",
    "with tf.Session() as sess:\n",
    "    # 执行计算图并获取张量的值\n",
    "    tensor_value = sess.run(tensor_v)\n",
    "\n",
    "# 打印张量的值\n",
    "print(tensor_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['x']\n",
    "t = train['t']\n",
    "lam=10\n",
    "its=10\n",
    "sq=False\n",
    "backpropT=False\n",
    "p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "t = tf.convert_to_tensor(t, dtype=tf.float32)\n",
    "it = tf.where(t>0)[:,0]\n",
    "ic = tf.where(t<1)[:,0]\n",
    "Xc = tf.gather(X,ic)\n",
    "Xt = tf.gather(X,it)\n",
    "nc = tf.to_float(tf.shape(Xc)[0])\n",
    "nt = tf.to_float(tf.shape(Xt)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute distance matrix'''\n",
    "if sq:\n",
    "    M = pdist2sq(Xt,Xc)\n",
    "else:\n",
    "    M = safe_sqrt(pdist2sq(Xt,Xc))\n",
    "\n",
    "''' Estimate lambda and delta '''\n",
    "M_mean = tf.reduce_mean(M)\n",
    "M_drop = tf.nn.dropout(M,10/(nc*nt))\n",
    "delta = tf.stop_gradient(tf.reduce_max(M))\n",
    "eff_lam = tf.stop_gradient(lam/M_mean)\n",
    "\n",
    "''' Compute new distance matrix '''\n",
    "Mt = M\n",
    "row = delta*tf.ones(tf.shape(M[0:1,:]))\n",
    "col = tf.concat([delta*tf.ones(tf.shape(M[:,0:1])),tf.zeros((1,1))],0)\n",
    "Mt = tf.concat([M,row],0)\n",
    "Mt = tf.concat([Mt,col],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute marginal vectors '''\n",
    "a = tf.concat([p*tf.ones((10000,1))/nt, (1-p)*tf.ones((1,1))],0)\n",
    "b = tf.concat([(1-p)*tf.ones((10000,1))/nc, p*tf.ones((1,1))],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compute marginal vectors '''\n",
    "a = tf.concat([p*tf.ones(tf.shape(tf.where(t>0)[:,0:1]))/nt, (1-p)*tf.ones((1,1))],0)\n",
    "b = tf.concat([(1-p)*tf.ones(tf.shape(tf.where(t<1)[:,0:1]))/nc, p*tf.ones((1,1))],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5384, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 假设有一个名为 tensor 的 TensorFlow 张量\n",
    "tensor_v = a\n",
    "# 创建 TensorFlow 会话\n",
    "with tf.Session() as sess:\n",
    "    # 执行计算图并获取张量的值\n",
    "    tensor_value = sess.run(tensor_v)\n",
    "\n",
    "# 打印张量的值\n",
    "print(tensor_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
