{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(r\"../\")\n",
    "\n",
    "dataDir='../Data/Causal/'\n",
    "storage_path='../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SynOOD_1_0_1_0.05_0.05_1\n",
      "##############################\n",
      "The data path is: ../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "The ATE:\n",
      "------------------------------\n",
      "ate: 1\n",
      "------------------------------\n",
      "Next, run dataGenerator: \n",
      "Run 0/10. \n",
      "Run 1/10. \n",
      "Run 2/10. \n",
      "Run 3/10. \n",
      "Run 4/10. \n",
      "Run 5/10. \n",
      "Run 6/10. \n",
      "Run 7/10. \n",
      "Run 8/10. \n",
      "Run 9/10. \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils import Syn_Generator_OOD\n",
    "\n",
    "Syn_2442 = Syn_Generator_OOD(n=1000, ate=0,sc=1,sh=0,one=1,depX=0.05,depU=0.05,VX=1,mV=2,mX=4,mU=4,mXs=2,init_seed=7,seed_coef=10,details=1,storage_path=storage_path)\n",
    "Syn_2442.run(n=1000, num_reps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets = [Syn_2442]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import log, CausalDataset\n",
    "from module.Regression_OOD import run as run_Reg\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def get_args():\n",
    "    argparser = argparse.ArgumentParser(description=__doc__)\n",
    "    # About run setting !!!!\n",
    "    argparser.add_argument('--seed',default=2021,type=int,help='The random seed')\n",
    "    argparser.add_argument('--mode',default='vx',type=str,help='The choice of v/x/vx/xx')\n",
    "    argparser.add_argument('--rewrite_log',default=False,type=bool,help='Whether rewrite log file')\n",
    "    argparser.add_argument('--use_gpu',default=True,type=bool,help='The use of GPU')\n",
    "    # About data setting ~~~~\n",
    "    argparser.add_argument('--num',default=10000,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--num_reps',default=10,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--ate',default=0,type=float,help='The ate of constant')\n",
    "    argparser.add_argument('--mV',default=2,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mX',default=4,type=int,help='The dim of Confounding variables X')\n",
    "    argparser.add_argument('--mU',default=4,type=int,help='The dim of Unobserved confounding variables U')\n",
    "    argparser.add_argument('--mXs',default=2,type=int,help='The dim of Noise variables D')\n",
    "    argparser.add_argument('--storage_path',default='../Data/',type=str,help='The dir of data storage')\n",
    "    # About Regression_t\n",
    "    argparser.add_argument('--regt_batch_size',default=500,type=int,help='The size of one batch')\n",
    "    argparser.add_argument('--regt_lr',default=0.05,type=float,help='The learning rate')\n",
    "    argparser.add_argument('--regt_num_epoch',default=3,type=int,help='The num of total epoch')\n",
    "    args = argparser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 0 :this is the 0/3 epochs.\n",
      "The train accuracy: 76.60 %\n",
      "The test  accuracy: 75.50 %\n",
      "Exp 0 :this is the 1/3 epochs.\n",
      "The train accuracy: 77.50 %\n",
      "The test  accuracy: 76.50 %\n",
      "Exp 0 :this is the 2/3 epochs.\n",
      "The train accuracy: 77.80 %\n",
      "The test  accuracy: 77.30 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 1 :this is the 0/3 epochs.\n",
      "The train accuracy: 74.70 %\n",
      "The test  accuracy: 75.60 %\n",
      "Exp 1 :this is the 1/3 epochs.\n",
      "The train accuracy: 75.40 %\n",
      "The test  accuracy: 77.60 %\n",
      "Exp 1 :this is the 2/3 epochs.\n",
      "The train accuracy: 76.80 %\n",
      "The test  accuracy: 78.10 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 2 :this is the 0/3 epochs.\n",
      "The train accuracy: 75.30 %\n",
      "The test  accuracy: 73.70 %\n",
      "Exp 2 :this is the 1/3 epochs.\n",
      "The train accuracy: 78.10 %\n",
      "The test  accuracy: 74.20 %\n",
      "Exp 2 :this is the 2/3 epochs.\n",
      "The train accuracy: 79.60 %\n",
      "The test  accuracy: 75.50 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 3 :this is the 0/3 epochs.\n",
      "The train accuracy: 75.60 %\n",
      "The test  accuracy: 75.10 %\n",
      "Exp 3 :this is the 1/3 epochs.\n",
      "The train accuracy: 77.80 %\n",
      "The test  accuracy: 75.10 %\n",
      "Exp 3 :this is the 2/3 epochs.\n",
      "The train accuracy: 78.20 %\n",
      "The test  accuracy: 75.10 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 4 :this is the 0/3 epochs.\n",
      "The train accuracy: 73.50 %\n",
      "The test  accuracy: 71.00 %\n",
      "Exp 4 :this is the 1/3 epochs.\n",
      "The train accuracy: 76.20 %\n",
      "The test  accuracy: 73.60 %\n",
      "Exp 4 :this is the 2/3 epochs.\n",
      "The train accuracy: 76.90 %\n",
      "The test  accuracy: 75.00 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 5 :this is the 0/3 epochs.\n",
      "The train accuracy: 71.30 %\n",
      "The test  accuracy: 74.60 %\n",
      "Exp 5 :this is the 1/3 epochs.\n",
      "The train accuracy: 73.50 %\n",
      "The test  accuracy: 76.70 %\n",
      "Exp 5 :this is the 2/3 epochs.\n",
      "The train accuracy: 74.50 %\n",
      "The test  accuracy: 77.40 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 6 :this is the 0/3 epochs.\n",
      "The train accuracy: 76.50 %\n",
      "The test  accuracy: 75.80 %\n",
      "Exp 6 :this is the 1/3 epochs.\n",
      "The train accuracy: 78.00 %\n",
      "The test  accuracy: 78.10 %\n",
      "Exp 6 :this is the 2/3 epochs.\n",
      "The train accuracy: 78.80 %\n",
      "The test  accuracy: 78.50 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 7 :this is the 0/3 epochs.\n",
      "The train accuracy: 77.00 %\n",
      "The test  accuracy: 73.40 %\n",
      "Exp 7 :this is the 1/3 epochs.\n",
      "The train accuracy: 78.80 %\n",
      "The test  accuracy: 75.00 %\n",
      "Exp 7 :this is the 2/3 epochs.\n",
      "The train accuracy: 79.40 %\n",
      "The test  accuracy: 75.80 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 8 :this is the 0/3 epochs.\n",
      "The train accuracy: 74.50 %\n",
      "The test  accuracy: 72.40 %\n",
      "Exp 8 :this is the 1/3 epochs.\n",
      "The train accuracy: 75.20 %\n",
      "The test  accuracy: 74.50 %\n",
      "Exp 8 :this is the 2/3 epochs.\n",
      "The train accuracy: 76.70 %\n",
      "The test  accuracy: 75.20 %\n",
      "../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/\n",
      "Exp 9 :this is the 0/3 epochs.\n",
      "The train accuracy: 72.80 %\n",
      "The test  accuracy: 74.60 %\n",
      "Exp 9 :this is the 1/3 epochs.\n",
      "The train accuracy: 75.00 %\n",
      "The test  accuracy: 77.20 %\n",
      "Exp 9 :this is the 2/3 epochs.\n",
      "The train accuracy: 76.00 %\n",
      "The test  accuracy: 77.80 %\n"
     ]
    }
   ],
   "source": [
    "# run vx\n",
    "for data in Datasets:\n",
    "    which_benchmark = data.which_benchmark\n",
    "    which_dataset = data.which_dataset\n",
    "    args.num_reps = 10\n",
    "    args.mV = data.mV\n",
    "    args.mX = data.mX\n",
    "    args.mU = data.mU\n",
    "    args.mXs = data.mXs\n",
    "    args.mode = 'vx'\n",
    "\n",
    "    resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}/'\n",
    "    dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "    os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "\n",
    "    if args.rewrite_log:\n",
    "        f = open(logfile,'w')\n",
    "        f.close()\n",
    "\n",
    "    for exp in range(args.num_reps):\n",
    "        train_df = pd.read_csv(dataDir + f'{exp}/train.csv')\n",
    "        print(dataDir)\n",
    "        val_df = pd.read_csv(dataDir + f'{exp}/val.csv')\n",
    "        test_df = pd.read_csv(dataDir + f'{exp}/test.csv')\n",
    "                                                    \n",
    "        train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "        val = CausalDataset(val_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "        test = CausalDataset(test_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "\n",
    "        train,val,test = run_Reg(exp, args, dataDir, resultDir, train, val, test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Datasets[0]\n",
    "which_benchmark = data.which_benchmark\n",
    "which_dataset = data.which_dataset\n",
    "args.num_reps = 10\n",
    "args.mV = data.mV\n",
    "args.mX = data.mX\n",
    "args.mU = data.mU\n",
    "args.mD = data.mXs\n",
    "args.mode = 'vx'\n",
    "\n",
    "resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}/'\n",
    "dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "logfile = f'{resultDir}/log.txt'\n",
    "\n",
    "train_df = pd.read_csv(dataDir + f'{0}/train.csv')\n",
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append(r\"../\")\n",
    "sys.path.append('/home/wyliu/code/CB-IV')\n",
    "from utils import log, CausalDataset\n",
    "from module.SynCBIV import run as run_SynCBIV\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def get_args():\n",
    "    argparser = argparse.ArgumentParser(description=__doc__)\n",
    "    # About run setting !!!!\n",
    "    argparser.add_argument('--seed',default=2021,type=int,help='The random seed')\n",
    "    argparser.add_argument('--mode',default='vx',type=str,help='The choice of v/x/vx/xx')\n",
    "    argparser.add_argument('--rewrite_log',default=False,type=bool,help='Whether rewrite log file')\n",
    "    argparser.add_argument('--use_gpu',default=False,type=bool,help='The use of GPU')\n",
    "    # About data setting ~~~~\n",
    "    argparser.add_argument('--num',default=10000,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--num_reps',default=10,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--ate',default=0,type=float,help='The ate of constant')\n",
    "    argparser.add_argument('--sc',default=1,type=float,help='The sc')\n",
    "    argparser.add_argument('--sh',default=0,type=float,help='The sh')\n",
    "    argparser.add_argument('--one',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--depX',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--depU',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--VX',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mV',default=2,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mX',default=4,type=int,help='The dim of Confounding variables X')\n",
    "    argparser.add_argument('--mU',default=4,type=int,help='The dim of Unobserved confounding variables U')\n",
    "    argparser.add_argument('--mXs',default=2,type=int,help='The dim of Noise variables D')\n",
    "    argparser.add_argument('--storage_path',default='../Data/',type=str,help='The dir of data storage')\n",
    "    # Syn\n",
    "    argparser.add_argument('--syn_alpha',default=0.01,type=float,help='')\n",
    "    argparser.add_argument('--syn_lambda',default=0.0001,type=float,help='')\n",
    "    argparser.add_argument('--syn_twoStage',default=True,type=bool,help='')\n",
    "    # About Debug or Show\n",
    "    argparser.add_argument('--verbose',default=1,type=int,help='The level of verbose')\n",
    "    argparser.add_argument('--epoch_show',default=5,type=int,help='The epochs of show time')\n",
    "    args = argparser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "which_benchmark = 'SynOOD_'+'_'.join(str(item) for item in [args.sc, args.sh, args.one, args.depX, args.depU,args.VX])\n",
    "which_dataset = '_'.join(str(item) for item in [args.mV, args.mX, args.mU, args.mXs])\n",
    "resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}/'\n",
    "dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "logfile = f'{resultDir}/log.txt'\n",
    "exp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Data//data/SynOOD_1_0_1_0.05_0.05_1/2_4_4_2/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' bias rate '''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 1.3, 1.5, 2.0, 2.5, 3.0, 0.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n",
      "-3.0\n",
      "-2.5\n",
      "-2.0\n",
      "-1.5\n",
      "-1.3\n",
      "1.3\n",
      "1.5\n",
      "2.0\n",
      "2.5\n",
      "3.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from utils import * \n",
    "for perm in range(args.num_reps):\n",
    "    for r in br:\n",
    "        print(r)\n",
    "        train_df = pd.read_csv(dataDir + f'{perm}/{args.mode}/train.csv')\n",
    "        val_df = pd.read_csv(dataDir + f'{perm}/{args.mode}/val.csv')\n",
    "        test_df = pd.read_csv(dataDir + f'{perm}/{args.mode}/test.csv')\n",
    "\n",
    "        train_df_ood = correlation_sample(train_df, r, 1000, args.mXs)\n",
    "        val_df_ood = correlation_sample(val_df, r, 1000, args.mXs)\n",
    "        test_df_ood = correlation_sample(test_df, r, 1000, args.mXs)\n",
    "\n",
    "        path = dataDir + '/{}/{}/'.format(perm, args.mode)\n",
    "        os.makedirs(os.path.dirname(path + f'ood_{brdc[r]}/'), exist_ok=True)\n",
    "\n",
    "        train_df_ood.to_csv(path + f'ood_{brdc[r]}/train.csv', index=False)\n",
    "        val_df_ood.to_csv(path + f'ood_{brdc[r]}/val.csv', index=False)\n",
    "        test_df_ood.to_csv(path + f'ood_{brdc[r]}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * \n",
    "r = 1.0\n",
    "# for perm in range(args.num_reps):\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/{args.mode}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/{args.mode}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/{args.mode}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df\n",
    "n = 1000\n",
    "dim_xs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nall = data.shape[0]\n",
    "prob = np.ones(nall)\n",
    "\n",
    "ite = data['m1']-data['m0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(range(nall), n, p=prob, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idv = 0\n",
    "d = np.abs(data[f'xs{idv}'] - np.sign(r) * ite) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000004"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
