{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torch.autograd import grad\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append(r\"../\")\n",
    "sys.path.append(r\"../../\")\n",
    "sys.path.append('/home/wyliu/code/CB-IV')\n",
    "from utils import log, CausalDataset\n",
    "from module.SynCBIV import run as run_SynCBIV\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def get_args():\n",
    "    argparser = argparse.ArgumentParser(description=__doc__)\n",
    "    # About run setting !!!!\n",
    "    argparser.add_argument('--seed',default=2021,type=int,help='The random seed')\n",
    "    argparser.add_argument('--mode',default='vx',type=str,help='The choice of v/x/vx/xx')\n",
    "    argparser.add_argument('--ood',default=-3.0,type=float,help='The train dataset of OOD')\n",
    "    argparser.add_argument('--ood_test',default=3.0,type=float,help='The train dataset of OOD')\n",
    "    argparser.add_argument('--rewrite_log',default=False,type=bool,help='Whether rewrite log file')\n",
    "    argparser.add_argument('--use_gpu',default=1,type=int,help='The use of GPU')\n",
    "    argparser.add_argument('--des_str',default='/_/',type=str,help='The description of this running')\n",
    "    argparser.add_argument('--oodtestall',default=0,type=int,help='The random seed')\n",
    "    argparser.add_argument('--iter',default=3000,type=int,help='The num of iterations')\n",
    "    # About data setting ~~~~\n",
    "    argparser.add_argument('--num',default=10000,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--num_reps',default=100,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--ate',default=0,type=float,help='The ate of constant')\n",
    "    argparser.add_argument('--sc',default=1,type=float,help='The sc')\n",
    "    argparser.add_argument('--sh',default=0,type=float,help='The sh')\n",
    "    argparser.add_argument('--one',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--depX',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--depU',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--VX',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mV',default=2,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mX',default=10,type=int,help='The dim of Confounding variables X')\n",
    "    argparser.add_argument('--mU',default=4,type=int,help='The dim of Unobserved confounding variables U')\n",
    "    argparser.add_argument('--mXs',default=2,type=int,help='The dim of Noise variables X')\n",
    "    argparser.add_argument('--storage_path',default='../../Data/',type=str,help='The dir of data storage')\n",
    "    # Syn\n",
    "    argparser.add_argument('--syn_alpha',default=0.01,type=float,help='')\n",
    "    argparser.add_argument('--syn_lambda',default=0.001,type=float,help='')\n",
    "    argparser.add_argument('--syn_twoStage',default=True,type=bool,help='')\n",
    "    argparser.add_argument('--lrate',default=0.001,type=float,help='learning rate')\n",
    "    # About Debug or Show\n",
    "    argparser.add_argument('--verbose',default=1,type=int,help='The level of verbose')\n",
    "    argparser.add_argument('--epoch_show',default=5,type=int,help='The epochs of show time')\n",
    "    # About Regression_t\n",
    "    argparser.add_argument('--regt_batch_size',default=500,type=int,help='The size of one batch')\n",
    "    argparser.add_argument('--regt_lr',default=0.1,type=float,help='The learning rate')\n",
    "    argparser.add_argument('--regt_num_epoch',default=5,type=int,help='The num of total epoch')\n",
    "    # About IRM  \n",
    "    argparser.add_argument('--env_list',default=[-3.0, -1.5, 1.5],type=list,help='The environment list')\n",
    "    argparser.add_argument('--data_dict',default={},type=dict,help='The data dict')\n",
    "    # args = argparser.parse_args()\n",
    "    args = argparser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import set_seed, log\n",
    "\n",
    "def get_gain(activation):\n",
    "    if activation.__class__.__name__ == \"LeakyReLU\":\n",
    "        gain = nn.init.calculate_gain(\"leaky_relu\",\n",
    "                                            activation.negative_slope)\n",
    "    else:\n",
    "        activation_name = activation.__class__.__name__.lower()\n",
    "        try:\n",
    "            gain = nn.init.calculate_gain(activation_name)\n",
    "        except ValueError:\n",
    "            gain = 1.0\n",
    "    return gain\n",
    "\n",
    "# input_dim：输入数据的维度。\n",
    "# layer_widths：一个整数列表，表示隐藏层的宽度。\n",
    "# activation：激活函数（默认为 None）。\n",
    "# last_layer：可选的最后一层，可以是任何 nn.Module 的子类（默认为 None）。\n",
    "# num_out：输出的维度（默认为 1）。\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_widths, activation=None,last_layer=None, num_out=1):\n",
    "        nn.Module.__init__(self)\n",
    "        self.gain=get_gain(activation)\n",
    "        # 根据隐藏层的宽度列表 layer_widths，\n",
    "        # 创建一系列的线性层（nn.Linear），\n",
    "        # 并可选择地在每个线性层之后添加给定的激活函数 activation。\n",
    "        # 最后，根据输出维度 num_out 添加最后一层线性层。\n",
    "        if len(layer_widths) == 0:\n",
    "            layers = [nn.Linear(input_dim, num_out)]\n",
    "        else:\n",
    "            num_layers = len(layer_widths)\n",
    "            if activation is None:\n",
    "                layers = [nn.Linear(input_dim, layer_widths[0])]\n",
    "            else:\n",
    "                layers = [nn.Linear(input_dim, layer_widths[0]), activation]\n",
    "            for i in range(1, num_layers):\n",
    "                w_in = layer_widths[i-1]\n",
    "                w_out = layer_widths[i]\n",
    "                if activation is None:\n",
    "                    layers.extend([nn.Linear(w_in, w_out)])\n",
    "                else:\n",
    "                    layers.extend([nn.Linear(w_in, w_out), activation])\n",
    "            layers.append(nn.Linear(layer_widths[-1], num_out))\n",
    "        if last_layer:\n",
    "            layers.append(last_layer)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def initialize(self, gain=1.0):\n",
    "        # initialize 方法用于初始化模型的参数。\n",
    "        for layer in self.model[:-1]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight.data, gain=self.gain)\n",
    "                nn.init.zeros_(layer.bias.data)\n",
    "        final_layer = self.model[-1]\n",
    "        nn.init.xavier_normal_(final_layer.weight.data, gain=gain)\n",
    "        nn.init.zeros_(final_layer.bias.data)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print(\"forward\", data.shape)\n",
    "        num_data = data.shape[0]\n",
    "        data = data.view(num_data, -1)\n",
    "        return self.model(data)\n",
    "\n",
    "class MultipleMLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_widths, num_models=1, activation=None,last_layer=None, num_out=1):\n",
    "        nn.Module.__init__(self)\n",
    "        self.models = nn.ModuleList([MLPModel(\n",
    "            input_dim, layer_widths, activation=activation,\n",
    "            last_layer=last_layer, num_out=num_out) for _ in range(num_models)])\n",
    "        self.num_models = num_models\n",
    "\n",
    "    def forward(self, data):\n",
    "        num_data = data.shape[0]\n",
    "        data = data.view(num_data, -1)\n",
    "        outputs = [self.models[i](data) for i in range(self.num_models)]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "def run(exp, args, dataDir, resultDir, train, val, test, device, r):\n",
    "    batch_size = args.regt_batch_size\n",
    "    print('args.regt_lr ',args.regt_lr)\n",
    "    lr = args.regt_lr\n",
    "    num_epoch = args.regt_num_epoch\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "    _logfile = f'{resultDir}/Regression.txt'\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        train.to_tensor()\n",
    "        val.to_tensor()\n",
    "        test.to_tensor()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size)\n",
    "    if args.mode == 'v':\n",
    "        input_dim = args.mV\n",
    "        train_input = train.v\n",
    "        val_input = val.v\n",
    "        test_input = test.v\n",
    "    elif args.mode == 'x':\n",
    "        input_dim = args.mX + args.mXs\n",
    "        train_input = torch.cat((train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.x, test.xs),1)\n",
    "    else:\n",
    "        input_dim = args.mV + args.mX + args.mXs\n",
    "        # print(\"input dim:\", input_dim)\n",
    "        train_input = torch.cat((train.v, train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.v, val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.v, test.x, test.xs),1)\n",
    "\n",
    "    \n",
    "    mlp = MLPModel(input_dim, layer_widths=[128, 64], activation=nn.ReLU(),last_layer=nn.BatchNorm1d(2), num_out=2)\n",
    "    net = nn.Sequential(mlp)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        log(logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\")\n",
    "        log(_logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\", False)\n",
    "        for idx, inputs in enumerate(train_loader):\n",
    "            u = inputs['u']\n",
    "            v = inputs['v']\n",
    "            x = torch.cat((inputs['x'], inputs['xs']), 1)\n",
    "            t = inputs['t'].reshape(-1).type(torch.LongTensor)\n",
    "            # print(\"x:\", x.shape)\n",
    "            # print(\"args.mode:\",args.mode)\n",
    "            if args.mode == 'v':\n",
    "                input_batch = v\n",
    "            elif args.mode == 'x':\n",
    "                input_batch = x\n",
    "                # print(\"input_batch:\", input_batch.shape)\n",
    "            else:\n",
    "                input_batch = torch.cat((v, x),1)\n",
    "            \n",
    "            prediction = net(input_batch) \n",
    "            loss = loss_func(prediction, t)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            loss.backward()        \n",
    "            optimizer.step()    \n",
    "\n",
    "        log(logfile, 'The train accuracy: {:.2f} %'.format((torch.true_divide(sum(train.t.reshape(-1) == torch.max(F.softmax(net(train_input) , dim=1), 1)[1]), len(train.t))).item() * 100))\n",
    "        log(_logfile, 'The test  accuracy: {:.2f} %'.format((torch.true_divide(sum(test.t.reshape(-1) == torch.max(F.softmax(net(test_input) , dim=1), 1)[1]), len(test.t))).item() * 100))\n",
    "\n",
    "    train.s = F.softmax(net(train_input) , dim=1)[:,1:2]\n",
    "    val.s = F.softmax(net(val_input) , dim=1)[:,1:2]\n",
    "    test.s = F.softmax(net(test_input) , dim=1)[:,1:2]\n",
    "    ''' bias rate 1'''\n",
    "    br = [-3.0, -2.5, -2.0, -1.5, -1.3, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "    brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "\n",
    "    return train,val,test\n",
    "\n",
    "\n",
    "def run_ood_IRM(exp, args, dataDir, resultDir, train, val, test, ood_test_dict=None):\n",
    "    batch_size = args.regt_batch_size\n",
    "    lr = args.regt_lr\n",
    "    num_epoch = args.regt_num_epoch\n",
    "    len_loader = 0\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "    _logfile = f'{resultDir}/Regression.txt'\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        train.to_tensor()\n",
    "        val.to_tensor()\n",
    "        test.to_tensor()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if args.mode == 'v':\n",
    "        input_dim = args.mV\n",
    "        train_input = train.v\n",
    "        val_input = val.v\n",
    "        test_input = test.v\n",
    "    elif args.mode == 'x':\n",
    "        input_dim = args.mX + args.mXs\n",
    "        train_input = torch.cat((train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.x, test.xs),1)\n",
    "    else:\n",
    "        input_dim = args.mV + args.mX + args.mXs\n",
    "        # print(\"input dim:\", input_dim)\n",
    "        train_input = torch.cat((train.v, train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.v, val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.v, test.x, test.xs),1)\n",
    "\n",
    "    for r in args.env_list:\n",
    "        train_temp = args.data_dict[r]['train']\n",
    "        val_temp = args.data_dict[r]['val']\n",
    "        test_temp = args.data_dict[r]['test']\n",
    "\n",
    "        try:\n",
    "            train_temp.to_tensor()\n",
    "            val_temp.to_tensor()\n",
    "            test_temp.to_tensor()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        args.data_dict[r]['trainloader_reg'] = DataLoader(train_temp, batch_size=batch_size)\n",
    "        len_loader= len(args.data_dict[r]['trainloader_reg'])\n",
    "\n",
    "    mlp = MLPModel(input_dim, layer_widths=[128, 64], activation=nn.ReLU(),last_layer=nn.BatchNorm1d(2), num_out=2)\n",
    "    net = nn.Sequential(mlp)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    dummy_w = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "    reg = 1e-1\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        log(logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\")\n",
    "        log(_logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\", False)\n",
    "        train_loaders = [iter(args.data_dict[r]['trainloader_reg']) for r in args.env_list]\n",
    "        print(len(train_loaders))\n",
    "        for _ in range(len_loader):\n",
    "            print(_)\n",
    "            error = 0\n",
    "            penalty = 0\n",
    "            for loader in train_loaders:\n",
    "                inputs = next(loader, None)\n",
    "                print(\"1000\")\n",
    "                if inputs is None:\n",
    "                    print(\"error!\")\n",
    "                u = inputs['u']\n",
    "                v = inputs['v']\n",
    "                x = torch.cat((inputs['x'], inputs['xs']), 1)\n",
    "                t = inputs['t'].reshape(-1).type(torch.LongTensor)\n",
    "                # print(\"x:\", x.shape)\n",
    "                # print(\"args.mode:\",args.mode)\n",
    "                if args.mode == 'v':\n",
    "                    input_batch = v\n",
    "                elif args.mode == 'x':\n",
    "                    input_batch = x\n",
    "                    # print(\"input_batch:\", input_batch.shape)\n",
    "                else:\n",
    "                    input_batch = torch.cat((v, x),1)\n",
    "                \n",
    "                prediction = net(input_batch) \n",
    "                loss = loss_func(prediction * dummy_w, t)\n",
    "                error += loss.mean()\n",
    "                penalty += grad(loss.mean(), dummy_w,\n",
    "                                create_graph=True)[0].pow(2).mean()\n",
    "            optimizer.zero_grad()  \n",
    "            (reg * error + (1 - reg) * penalty).backward()      \n",
    "            optimizer.step()      \n",
    "\n",
    "        log(logfile, 'The train accuracy: {:.2f} %'.format((torch.true_divide(sum(train.t.reshape(-1) == torch.max(F.softmax(net(train_input) , dim=1), 1)[1]), len(train.t))).item() * 100))\n",
    "        log(_logfile, 'The test  accuracy: {:.2f} %'.format((torch.true_divide(sum(test.t.reshape(-1) == torch.max(F.softmax(net(test_input) , dim=1), 1)[1]), len(test.t))).item() * 100))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' bias rate '''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 1.3, 1.5, 2.0, 2.5, 3.0, 0.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "which_benchmark = 'SynOOD2_'+'_'.join(str(item) for item in [args.sc, args.sh, args.one, args.depX, args.depU,args.VX])\n",
    "which_dataset = '_'.join(str(item) for item in [args.mV, args.mX, args.mU, args.mXs])\n",
    "resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}_{args.mode}/ood{brdc[args.ood]}/'\n",
    "dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "logfile = f'{resultDir}/log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data//data/SynOOD2_1_0_1_0.05_0.05_1/2_10_4_2/0/ood_p30/vx/train.csv\n",
      "../../Data//data/SynOOD2_1_0_1_0.05_0.05_1/2_10_4_2/0/ood_n30/vx/test.csv\n"
     ]
    }
   ],
   "source": [
    "exp = 0\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')\n",
    "train_df2 = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-1.5]}/{args.mode}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/test.csv')\n",
    "# 合并 train 和 val 数据集\n",
    "combined_df = pd.concat([train_df, train_df2], ignore_index=True)\n",
    "# 打乱顺序\n",
    "combined_df = shuffle(combined_df)\n",
    "print(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')\n",
    "print(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/test.csv')\n",
    "# 创建新的数据集\n",
    "combined_dataset = CausalDataset(combined_df, variables=['v', 'u', 'x', 'xs', 'z', 'p', 's', 'm', 't', 'g', 'y', 'f', 'c'])\n",
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "val = CausalDataset(val_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "test = CausalDataset(test_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "# train,val,test = run(exp, args, dataDir, resultDir, train, val, test, device, r=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
       "       'u1', 'u2', 'u3', 'u4', 'xs1', 'xs2', 'z', 'pi', 't', 'mu0', 'mu1', 'y',\n",
       "       'f'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = 1\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/train.csv')\n",
    "# train = CausalDataset(train_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "train_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.5, 1.5]\n",
      "Exp 0 :this is the 0/5 epochs.\n",
      "3\n",
      "0\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "2\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "3\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "4\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "5\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "6\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "7\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "8\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "9\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "10\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "11\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "12\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "13\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "14\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "15\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "16\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "17\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "18\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "19\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "The train accuracy: 74.91 %\n",
      "The test  accuracy: 72.24 %\n",
      "Exp 0 :this is the 1/5 epochs.\n",
      "3\n",
      "0\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "2\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "3\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "4\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "5\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "6\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "7\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "8\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "9\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "10\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "11\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "12\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "13\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "14\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "15\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "16\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "17\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "18\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "19\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "The train accuracy: 77.31 %\n",
      "The test  accuracy: 75.24 %\n",
      "Exp 0 :this is the 2/5 epochs.\n",
      "3\n",
      "0\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "2\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "3\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "4\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "5\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "6\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "7\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "8\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "9\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "10\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "11\n",
      "1000\n",
      "1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-857b73f0410f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCausalDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'u'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'xs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mrun_ood_IRM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mood_test_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0bd0b3c5acaa>\u001b[0m in \u001b[0;36mrun_ood_IRM\u001b[0;34m(exp, args, dataDir, resultDir, train, val, test, ood_test_dict)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mpenalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/code/CB-IV/utils/loadDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mvar_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'var_dict[\\'{var}\\']=self.{var}[idx]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvar_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/code/CB-IV/utils/loadDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' OOD test'''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 0.0, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "exp = 0\n",
    "for r in br:\n",
    "    args.data_dict[r] = {\n",
    "        'train': None,\n",
    "        'val': None,\n",
    "        'test': None,\n",
    "        'trainloader_reg': None,\n",
    "        'env': 0,\n",
    "    }\n",
    "    if r in args.env_list:\n",
    "        args.data_dict[r]['env'] = 1\n",
    "        train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/train.csv')\n",
    "        val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/val.csv')\n",
    "        test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/test.csv')\n",
    "\n",
    "        args.data_dict[r]['train'] = CausalDataset(train_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        args.data_dict[r]['val'] = CausalDataset(val_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        args.data_dict[r]['test'] = CausalDataset(test_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "\n",
    "\n",
    "exp = 0\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-1.3]}/{args.mode}/train.csv')\n",
    "train_df2 = pd.read_csv(dataDir + f'{exp}/ood_{brdc[2.5]}/{args.mode}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/val.csv')\n",
    "# 合并 train 和 val 数据集\n",
    "combined_df = pd.concat([train_df, train_df2], ignore_index=True)\n",
    "# 打乱顺序\n",
    "combined_df = shuffle(combined_df)\n",
    "\n",
    "# 创建新的数据集\n",
    "combined_dataset = CausalDataset(combined_df, variables=['v', 'u', 'x', 'xs', 'z', 'p', 's', 'm', 't', 'g', 'y', 'f', 'c'])\n",
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "val = CausalDataset(val_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "test = CausalDataset(test_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "print(args.env_list)\n",
    "run_ood_IRM(exp, args, dataDir, resultDir, train, val, test, ood_test_dict=[]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = args.data_dict[-3.0]['env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_dict[-3.0]['env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 0\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 7000\n",
      "测试集大小: 2000\n",
      "验证集大小: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据集\n",
    "data = train_df\n",
    "\n",
    "# 划分训练集和剩余数据\n",
    "train_data, remaining_data = train_test_split(data, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "# 划分测试集和验证集\n",
    "test_data, val_data = train_test_split(remaining_data, test_size=(1/3), shuffle=True, random_state=42)\n",
    "\n",
    "# 打印划分后的数据集大小\n",
    "print(\"训练集大小:\", len(train_data))\n",
    "print(\"测试集大小:\", len(test_data))\n",
    "print(\"验证集大小:\", len(val_data))\n",
    "\n",
    "# 进行后续操作，使用划分后的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>u0</th>\n",
       "      <th>u1</th>\n",
       "      <th>u2</th>\n",
       "      <th>u3</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>...</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>c10</th>\n",
       "      <th>c11</th>\n",
       "      <th>c12</th>\n",
       "      <th>c13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9069</th>\n",
       "      <td>0.524600</td>\n",
       "      <td>-0.525552</td>\n",
       "      <td>0.481662</td>\n",
       "      <td>0.401454</td>\n",
       "      <td>1.898650</td>\n",
       "      <td>1.968174</td>\n",
       "      <td>0.978375</td>\n",
       "      <td>-0.341613</td>\n",
       "      <td>0.629651</td>\n",
       "      <td>0.082054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629651</td>\n",
       "      <td>0.082054</td>\n",
       "      <td>-0.924961</td>\n",
       "      <td>-0.518290</td>\n",
       "      <td>1.281546</td>\n",
       "      <td>0.598323</td>\n",
       "      <td>0.613425</td>\n",
       "      <td>0.756848</td>\n",
       "      <td>0.467906</td>\n",
       "      <td>0.222363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>1.214455</td>\n",
       "      <td>-0.751910</td>\n",
       "      <td>-0.811350</td>\n",
       "      <td>1.006010</td>\n",
       "      <td>-0.449178</td>\n",
       "      <td>0.515667</td>\n",
       "      <td>-0.471500</td>\n",
       "      <td>-0.193685</td>\n",
       "      <td>0.511216</td>\n",
       "      <td>0.235622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511216</td>\n",
       "      <td>0.235622</td>\n",
       "      <td>-0.512427</td>\n",
       "      <td>0.555847</td>\n",
       "      <td>-0.094491</td>\n",
       "      <td>-0.013939</td>\n",
       "      <td>1.032238</td>\n",
       "      <td>0.105693</td>\n",
       "      <td>0.325317</td>\n",
       "      <td>0.432817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7738</th>\n",
       "      <td>-0.671099</td>\n",
       "      <td>0.036636</td>\n",
       "      <td>0.227971</td>\n",
       "      <td>-0.396112</td>\n",
       "      <td>2.877801</td>\n",
       "      <td>0.635873</td>\n",
       "      <td>-0.349373</td>\n",
       "      <td>0.692276</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.229380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.229380</td>\n",
       "      <td>-0.101457</td>\n",
       "      <td>-1.053224</td>\n",
       "      <td>1.369451</td>\n",
       "      <td>0.272344</td>\n",
       "      <td>0.730052</td>\n",
       "      <td>-0.148678</td>\n",
       "      <td>0.866132</td>\n",
       "      <td>1.052830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>-1.863404</td>\n",
       "      <td>-0.307318</td>\n",
       "      <td>1.752581</td>\n",
       "      <td>0.183689</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>-1.531521</td>\n",
       "      <td>-0.795271</td>\n",
       "      <td>-1.164185</td>\n",
       "      <td>1.105108</td>\n",
       "      <td>-0.432358</td>\n",
       "      <td>...</td>\n",
       "      <td>1.105108</td>\n",
       "      <td>-0.432358</td>\n",
       "      <td>-0.586890</td>\n",
       "      <td>-0.518608</td>\n",
       "      <td>1.229404</td>\n",
       "      <td>0.649537</td>\n",
       "      <td>-0.490955</td>\n",
       "      <td>0.066665</td>\n",
       "      <td>1.139130</td>\n",
       "      <td>0.955422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>-0.705511</td>\n",
       "      <td>1.106909</td>\n",
       "      <td>-0.992365</td>\n",
       "      <td>-0.317162</td>\n",
       "      <td>1.583554</td>\n",
       "      <td>0.884582</td>\n",
       "      <td>-0.386704</td>\n",
       "      <td>0.997796</td>\n",
       "      <td>0.503503</td>\n",
       "      <td>-0.681674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503503</td>\n",
       "      <td>-0.681674</td>\n",
       "      <td>0.281742</td>\n",
       "      <td>0.384268</td>\n",
       "      <td>-0.628569</td>\n",
       "      <td>-0.117194</td>\n",
       "      <td>-0.575836</td>\n",
       "      <td>0.612674</td>\n",
       "      <td>0.679975</td>\n",
       "      <td>0.800622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>-0.428441</td>\n",
       "      <td>-1.260926</td>\n",
       "      <td>-1.175668</td>\n",
       "      <td>0.752964</td>\n",
       "      <td>-1.181665</td>\n",
       "      <td>-1.542833</td>\n",
       "      <td>-0.270879</td>\n",
       "      <td>0.104049</td>\n",
       "      <td>0.112153</td>\n",
       "      <td>-0.230140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112153</td>\n",
       "      <td>-0.230140</td>\n",
       "      <td>-1.179994</td>\n",
       "      <td>-0.036267</td>\n",
       "      <td>0.631529</td>\n",
       "      <td>0.602401</td>\n",
       "      <td>1.270601</td>\n",
       "      <td>-0.883364</td>\n",
       "      <td>0.836381</td>\n",
       "      <td>0.779818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>-0.221534</td>\n",
       "      <td>-0.403937</td>\n",
       "      <td>-0.472669</td>\n",
       "      <td>1.350004</td>\n",
       "      <td>-0.472227</td>\n",
       "      <td>0.822671</td>\n",
       "      <td>-1.263000</td>\n",
       "      <td>0.322743</td>\n",
       "      <td>-0.788642</td>\n",
       "      <td>-1.244502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788642</td>\n",
       "      <td>-1.244502</td>\n",
       "      <td>-1.398260</td>\n",
       "      <td>-0.039018</td>\n",
       "      <td>-1.279876</td>\n",
       "      <td>0.152198</td>\n",
       "      <td>-0.146648</td>\n",
       "      <td>-1.779244</td>\n",
       "      <td>1.645399</td>\n",
       "      <td>1.345456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0.902519</td>\n",
       "      <td>0.426075</td>\n",
       "      <td>0.611076</td>\n",
       "      <td>1.684176</td>\n",
       "      <td>1.037696</td>\n",
       "      <td>-0.433580</td>\n",
       "      <td>-0.121180</td>\n",
       "      <td>0.517547</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>0.664572</td>\n",
       "      <td>-0.807385</td>\n",
       "      <td>0.818987</td>\n",
       "      <td>0.415126</td>\n",
       "      <td>0.225441</td>\n",
       "      <td>-0.618124</td>\n",
       "      <td>0.206968</td>\n",
       "      <td>0.544344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1.659318</td>\n",
       "      <td>0.078774</td>\n",
       "      <td>0.930513</td>\n",
       "      <td>-0.303925</td>\n",
       "      <td>0.326301</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>-0.043024</td>\n",
       "      <td>-1.330140</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>-0.455293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>-0.455293</td>\n",
       "      <td>0.229716</td>\n",
       "      <td>0.383941</td>\n",
       "      <td>-0.910247</td>\n",
       "      <td>0.768532</td>\n",
       "      <td>0.665644</td>\n",
       "      <td>1.180827</td>\n",
       "      <td>0.082144</td>\n",
       "      <td>0.333329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>1.887944</td>\n",
       "      <td>0.356941</td>\n",
       "      <td>2.148641</td>\n",
       "      <td>-0.291734</td>\n",
       "      <td>0.709542</td>\n",
       "      <td>1.057250</td>\n",
       "      <td>0.648191</td>\n",
       "      <td>0.379612</td>\n",
       "      <td>0.070827</td>\n",
       "      <td>0.437266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070827</td>\n",
       "      <td>0.437266</td>\n",
       "      <td>0.609723</td>\n",
       "      <td>-0.477012</td>\n",
       "      <td>1.206010</td>\n",
       "      <td>0.502513</td>\n",
       "      <td>-0.866156</td>\n",
       "      <td>0.487778</td>\n",
       "      <td>-0.135604</td>\n",
       "      <td>0.207302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            v0        v1        u0        u1        u2        u3        x0  \\\n",
       "9069  0.524600 -0.525552  0.481662  0.401454  1.898650  1.968174  0.978375   \n",
       "2603  1.214455 -0.751910 -0.811350  1.006010 -0.449178  0.515667 -0.471500   \n",
       "7738 -0.671099  0.036636  0.227971 -0.396112  2.877801  0.635873 -0.349373   \n",
       "1579 -1.863404 -0.307318  1.752581  0.183689 -0.062972 -1.531521 -0.795271   \n",
       "5058 -0.705511  1.106909 -0.992365 -0.317162  1.583554  0.884582 -0.386704   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5734 -0.428441 -1.260926 -1.175668  0.752964 -1.181665 -1.542833 -0.270879   \n",
       "5191 -0.221534 -0.403937 -0.472669  1.350004 -0.472227  0.822671 -1.263000   \n",
       "5390  0.902519  0.426075  0.611076  1.684176  1.037696 -0.433580 -0.121180   \n",
       "860   1.659318  0.078774  0.930513 -0.303925  0.326301  0.635371 -0.043024   \n",
       "7270  1.887944  0.356941  2.148641 -0.291734  0.709542  1.057250  0.648191   \n",
       "\n",
       "            x1        x2        x3  ...        c4        c5        c6  \\\n",
       "9069 -0.341613  0.629651  0.082054  ...  0.629651  0.082054 -0.924961   \n",
       "2603 -0.193685  0.511216  0.235622  ...  0.511216  0.235622 -0.512427   \n",
       "7738  0.692276 -0.002813  0.229380  ... -0.002813  0.229380 -0.101457   \n",
       "1579 -1.164185  1.105108 -0.432358  ...  1.105108 -0.432358 -0.586890   \n",
       "5058  0.997796  0.503503 -0.681674  ...  0.503503 -0.681674  0.281742   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5734  0.104049  0.112153 -0.230140  ...  0.112153 -0.230140 -1.179994   \n",
       "5191  0.322743 -0.788642 -1.244502  ... -0.788642 -1.244502 -1.398260   \n",
       "5390  0.517547  0.117253 -0.191259  ...  0.117253 -0.191259  0.664572   \n",
       "860  -1.330140  0.505785 -0.455293  ...  0.505785 -0.455293  0.229716   \n",
       "7270  0.379612  0.070827  0.437266  ...  0.070827  0.437266  0.609723   \n",
       "\n",
       "            c7        c8        c9       c10       c11       c12       c13  \n",
       "9069 -0.518290  1.281546  0.598323  0.613425  0.756848  0.467906  0.222363  \n",
       "2603  0.555847 -0.094491 -0.013939  1.032238  0.105693  0.325317  0.432817  \n",
       "7738 -1.053224  1.369451  0.272344  0.730052 -0.148678  0.866132  1.052830  \n",
       "1579 -0.518608  1.229404  0.649537 -0.490955  0.066665  1.139130  0.955422  \n",
       "5058  0.384268 -0.628569 -0.117194 -0.575836  0.612674  0.679975  0.800622  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5734 -0.036267  0.631529  0.602401  1.270601 -0.883364  0.836381  0.779818  \n",
       "5191 -0.039018 -1.279876  0.152198 -0.146648 -1.779244  1.645399  1.345456  \n",
       "5390 -0.807385  0.818987  0.415126  0.225441 -0.618124  0.206968  0.544344  \n",
       "860   0.383941 -0.910247  0.768532  0.665644  1.180827  0.082144  0.333329  \n",
       "7270 -0.477012  1.206010  0.502513 -0.866156  0.487778 -0.135604  0.207302  \n",
       "\n",
       "[7000 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='../Data/Causal/'\n",
    "ihdp_train_path = dataDir + 'ihdp_npci_1-1000.test.npz'\n",
    "ihdp_train = np.load(ihdp_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 25, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihdp_train['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'xs1', 'xs2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['x' + str(i+1) for i in range(10)] + ['xs' + str(i+1) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except:\n",
    "    pass\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "def get_FLAGS():\n",
    "    ''' Define parameter flags '''\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "    \n",
    "    tf.app.flags.DEFINE_integer('lrate_decay_num', 100, \"\"\"NUM_ITERATIONS_PER_DECAY. \"\"\") \n",
    "    tf.app.flags.DEFINE_integer('seed', 2021, \"\"\"Seed. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('debug', 0, \"\"\"Debug mode. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('save_rep', 0, \"\"\"Save representations after training. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('output_csv',0,\"\"\"Whether to save a CSV file with the results\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('output_delay', 100, \"\"\"Number of iterations between log/loss outputs. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('x_key', 'x', \"\"\"Which key to use (x/xu/vxu)\"\"\") # 没出现\n",
    "    tf.app.flags.DEFINE_string('loss', 'l2', \"\"\"Which loss function to use (l1/l2/log)\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('n_in', 3, \"\"\"Number of representation layers. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_integer('n_out', 5, \"\"\"Number of regression layers. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_float('p_alpha', 1, \"\"\"Imbalance regularization param. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_float('p_lambda', 1e-4, \"\"\"Weight decay regularization parameter. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_integer('rep_weight_decay', 0, \"\"\"Whether to penalize representation layers with weight decay\"\"\")\n",
    "    tf.app.flags.DEFINE_float('dropout_in', 1.0, \"\"\"Input layers dropout keep rate. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('dropout_out', 1.0, \"\"\"Output layers dropout keep rate. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('nonlin', 'elu', \"\"\"Kind of non-linearity. Default relu. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('lrate', 5e-4, \"\"\"Learning rate. \"\"\") # done\n",
    "    tf.app.flags.DEFINE_float('decay', 0.3, \"\"\"RMSProp decay. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('batch_size', 256, \"\"\"Batch size. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('dim_in', 100, \"\"\"Pre-representation layer dimensions. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_integer('dim_out', 100, \"\"\"Post-representation layer dimensions. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_integer('batch_norm', 0, \"\"\"Whether to use batch normalization. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_string('normalization', 'none', \"\"\"How to normalize representation (after batch norm). none/bn_fixed/divide/project \"\"\")\n",
    "    tf.app.flags.DEFINE_float('rbf_sigma', 0.1, \"\"\"RBF MMD sigma \"\"\") # 固定\n",
    "    tf.app.flags.DEFINE_integer('experiments', 2, \"\"\"Number of experiments. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('iterations', 3000, \"\"\"Number of iterations. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('weight_init', 0.1, \"\"\"Weight initialization scale. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('lrate_decay', 0.97, \"\"\"Decay of learning rate every 100 iterations \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('wass_iterations', 10, \"\"\"Number of iterations in Wasserstein computation. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('wass_lambda', 10.0, \"\"\"Wasserstein lambda. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('wass_bpt', 1, \"\"\"Backprop through T matrix? \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('varsel', 0, \"\"\"Whether the first layer performs variable selection. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('outdir', '../Data/DRCFR/results/', \"\"\"Output directory. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('datadir', '../Data/DRCFR/data/Syn_1.0_1.0_0/2_4_4/', \"\"\"Data directory. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('dataform', 'train_0.csv', \"\"\"Training data filename form. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('data_val', 'val_0.csv', \"\"\"Valid data filename form. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('data_test', 'test_0.csv', \"\"\"Test data filename form. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('sparse', 0, \"\"\"Whether data is stored in sparse format (.x, .y). \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('repetitions', 1, \"\"\"Repetitions with different seed.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('use_p_correction', 0, \"\"\"Whether to use population size p(t) in mmd/disc/wass.\"\"\")\n",
    "    tf.app.flags.DEFINE_string('optimizer', 'Adam', \"\"\"Which optimizer to use. (RMSProp/Adagrad/GradientDescent/Adam)\"\"\")\n",
    "    tf.app.flags.DEFINE_string('imb_fun', 'mmd2_rbf', \"\"\"Which imbalance penalty to use (mmd_lin/mmd_rbf/mmd2_lin/mmd2_rbf/lindisc/wass). \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('pred_output_delay', 200, \"\"\"Number of iterations between prediction outputs. (-1 gives no intermediate output). \"\"\")\n",
    "    tf.app.flags.DEFINE_float('val_part', 0.3, \"\"\"Validation part. \"\"\")\n",
    "    tf.app.flags.DEFINE_boolean('split_output', 1, \"\"\"Whether to split output layers between treated and control. \"\"\") # todo\n",
    "    tf.app.flags.DEFINE_boolean('reweight_sample', 1, \"\"\"Whether to reweight sample for prediction loss with average treatment probability. \"\"\")\n",
    "    tf.app.flags.DEFINE_boolean('twoStage', 1, \"\"\"twoStage. \"\"\")\n",
    "    tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "    tf.app.flags.DEFINE_string('ip', '', 'kernel')\n",
    "    tf.app.flags.DEFINE_integer('mV', 2, \"\"\"The dim of Instrumental variables V.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('mX', 4, \"\"\"The dim of Confounding variables X.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('mU', 4, \"\"\"The dim of Unobserved confounding variables U.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('mXs', 2, \"\"\"The dim of spourious variables U.\"\"\")\n",
    "    tf.app.flags.DEFINE_float('ood', 0., \"\"\"ood. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('ood_test', 3.0, \"\"\"ood. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('num_reps', 10, \"\"\"The num of train\\val\\test dataset.\"\"\")\n",
    "    tf.app.flags.DEFINE_string('des_str', '/_/', 'The description of this running')\n",
    "    tf.app.flags.DEFINE_integer('use_gpu', 0, \"\"\"The use of GPU. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('oodtestall', 0, \"\"\"ood test all.\"\"\")\n",
    "    tf.app.flags.DEFINE_integer('iter', 300, \"\"\"Number of iterations. \"\"\")\n",
    "    tf.app.flags.DEFINE_float('regt_lr', 0.05, \"\"\"Validation part. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('regt_num_epoch', 300, \"\"\"Number of iterations. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('version', 1, \"\"\"Version. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('ivreg', 1, \"\"\"Version. \"\"\")\n",
    "    tf.app.flags.DEFINE_integer('start_reps', 0, \"\"\"The start of train\\val\\test dataset. \"\"\")\n",
    "    \n",
    "    # About IRM  \n",
    "    tf.app.flags.DEFINE_string('env_str', '[3.0, -3.0]', 'The environment list')\n",
    "\n",
    "    if FLAGS.sparse:\n",
    "        import scipy.sparse as sparse\n",
    "\n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = get_FLAGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.VX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 2021\n",
      "mode vx\n",
      "ood -3.0\n",
      "ood_test 3.0\n",
      "rewrite_log False\n",
      "use_gpu 1\n",
      "des_str /_/\n",
      "oodtestall 0\n",
      "iter 3000\n",
      "num 10000\n",
      "num_reps 100\n",
      "ate 0\n",
      "sc 1\n",
      "sh 0\n",
      "one 1\n",
      "depX 0.05\n",
      "depU 0.05\n",
      "VX 1\n",
      "mV 2\n",
      "mX 10\n",
      "mU 4\n",
      "mXs 2\n",
      "storage_path ../../Data/\n",
      "syn_alpha 0.01\n",
      "syn_lambda 0.001\n",
      "syn_twoStage True\n",
      "lrate 0.001\n",
      "verbose 1\n",
      "epoch_show 5\n",
      "regt_batch_size 500\n",
      "regt_lr 0.1\n",
      "regt_num_epoch 5\n",
      "env_list [-3.0, -1.5, 1.5]\n",
      "data_dict {}\n"
     ]
    }
   ],
   "source": [
    "for key, value in vars(args).items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'lrate_decay_num' is defined twice. First from /home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: NUM_ITERATIONS_PER_DECAY. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-39b4741df079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_FLAGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7c94387a48a0>\u001b[0m in \u001b[0;36mget_FLAGS\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lrate_decay_num'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\"NUM_ITERATIONS_PER_DECAY. \"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2021\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\"Seed. \"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'debug'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\"Debug mode. \"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/tensorflow_core/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[0;34m(name, default, help, lower_bound, upper_bound, flag_values, required, **args)\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequired\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       **args)\n\u001b[0m\u001b[1;32m    434\u001b[0m   \u001b[0m_register_bounds_validator_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, required, **args)\u001b[0m\n\u001b[1;32m    100\u001b[0m   return DEFINE_flag(\n\u001b[1;32m    101\u001b[0m       \u001b[0m_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m       module_name, required)\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name, required)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'lrate_decay_num' is defined twice. First from /home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/ipykernel_launcher.py, Second from /home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/ipykernel_launcher.py.  Description from first occurrence: NUM_ITERATIONS_PER_DECAY. "
     ]
    }
   ],
   "source": [
    "FLAGS = get_FLAGS()\n",
    "\n",
    "for key, value in FLAGS.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
