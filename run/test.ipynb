{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from torch.autograd import grad\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append(r\"../\")\n",
    "sys.path.append(r\"../../\")\n",
    "sys.path.append('/home/wyliu/code/CB-IV')\n",
    "from utils import log, CausalDataset\n",
    "from module.SynCBIV import run as run_SynCBIV\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def get_args():\n",
    "    argparser = argparse.ArgumentParser(description=__doc__)\n",
    "    # About run setting !!!!\n",
    "    argparser.add_argument('--seed',default=2021,type=int,help='The random seed')\n",
    "    argparser.add_argument('--mode',default='vx',type=str,help='The choice of v/x/vx/xx')\n",
    "    argparser.add_argument('--ood',default=-3.0,type=float,help='The train dataset of OOD')\n",
    "    argparser.add_argument('--ood_test',default=3.0,type=float,help='The train dataset of OOD')\n",
    "    argparser.add_argument('--rewrite_log',default=False,type=bool,help='Whether rewrite log file')\n",
    "    argparser.add_argument('--use_gpu',default=1,type=int,help='The use of GPU')\n",
    "    argparser.add_argument('--des_str',default='/_/',type=str,help='The description of this running')\n",
    "    argparser.add_argument('--oodtestall',default=0,type=int,help='The random seed')\n",
    "    argparser.add_argument('--iter',default=3000,type=int,help='The num of iterations')\n",
    "    # About data setting ~~~~\n",
    "    argparser.add_argument('--num',default=10000,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--num_reps',default=100,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--ate',default=0,type=float,help='The ate of constant')\n",
    "    argparser.add_argument('--sc',default=1,type=float,help='The sc')\n",
    "    argparser.add_argument('--sh',default=0,type=float,help='The sh')\n",
    "    argparser.add_argument('--one',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--depX',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--depU',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--VX',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mV',default=2,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mX',default=10,type=int,help='The dim of Confounding variables X')\n",
    "    argparser.add_argument('--mU',default=4,type=int,help='The dim of Unobserved confounding variables U')\n",
    "    argparser.add_argument('--mXs',default=2,type=int,help='The dim of Noise variables X')\n",
    "    argparser.add_argument('--storage_path',default='../../Data/',type=str,help='The dir of data storage')\n",
    "    # Syn\n",
    "    argparser.add_argument('--syn_alpha',default=0.01,type=float,help='')\n",
    "    argparser.add_argument('--syn_lambda',default=0.001,type=float,help='')\n",
    "    argparser.add_argument('--syn_twoStage',default=True,type=bool,help='')\n",
    "    argparser.add_argument('--lrate',default=0.001,type=float,help='learning rate')\n",
    "    # About Debug or Show\n",
    "    argparser.add_argument('--verbose',default=1,type=int,help='The level of verbose')\n",
    "    argparser.add_argument('--epoch_show',default=5,type=int,help='The epochs of show time')\n",
    "    # About Regression_t\n",
    "    argparser.add_argument('--regt_batch_size',default=500,type=int,help='The size of one batch')\n",
    "    argparser.add_argument('--regt_lr',default=0.1,type=float,help='The learning rate')\n",
    "    argparser.add_argument('--regt_num_epoch',default=5,type=int,help='The num of total epoch')\n",
    "    # About IRM  \n",
    "    argparser.add_argument('--env_list',default=[-3.0, -1.5, 1.5],type=list,help='The environment list')\n",
    "    argparser.add_argument('--data_dict',default={},type=dict,help='The data dict')\n",
    "    # args = argparser.parse_args()\n",
    "    args = argparser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import set_seed, log\n",
    "\n",
    "def get_gain(activation):\n",
    "    if activation.__class__.__name__ == \"LeakyReLU\":\n",
    "        gain = nn.init.calculate_gain(\"leaky_relu\",\n",
    "                                            activation.negative_slope)\n",
    "    else:\n",
    "        activation_name = activation.__class__.__name__.lower()\n",
    "        try:\n",
    "            gain = nn.init.calculate_gain(activation_name)\n",
    "        except ValueError:\n",
    "            gain = 1.0\n",
    "    return gain\n",
    "\n",
    "# input_dim：输入数据的维度。\n",
    "# layer_widths：一个整数列表，表示隐藏层的宽度。\n",
    "# activation：激活函数（默认为 None）。\n",
    "# last_layer：可选的最后一层，可以是任何 nn.Module 的子类（默认为 None）。\n",
    "# num_out：输出的维度（默认为 1）。\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_widths, activation=None,last_layer=None, num_out=1):\n",
    "        nn.Module.__init__(self)\n",
    "        self.gain=get_gain(activation)\n",
    "        # 根据隐藏层的宽度列表 layer_widths，\n",
    "        # 创建一系列的线性层（nn.Linear），\n",
    "        # 并可选择地在每个线性层之后添加给定的激活函数 activation。\n",
    "        # 最后，根据输出维度 num_out 添加最后一层线性层。\n",
    "        if len(layer_widths) == 0:\n",
    "            layers = [nn.Linear(input_dim, num_out)]\n",
    "        else:\n",
    "            num_layers = len(layer_widths)\n",
    "            if activation is None:\n",
    "                layers = [nn.Linear(input_dim, layer_widths[0])]\n",
    "            else:\n",
    "                layers = [nn.Linear(input_dim, layer_widths[0]), activation]\n",
    "            for i in range(1, num_layers):\n",
    "                w_in = layer_widths[i-1]\n",
    "                w_out = layer_widths[i]\n",
    "                if activation is None:\n",
    "                    layers.extend([nn.Linear(w_in, w_out)])\n",
    "                else:\n",
    "                    layers.extend([nn.Linear(w_in, w_out), activation])\n",
    "            layers.append(nn.Linear(layer_widths[-1], num_out))\n",
    "        if last_layer:\n",
    "            layers.append(last_layer)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def initialize(self, gain=1.0):\n",
    "        # initialize 方法用于初始化模型的参数。\n",
    "        for layer in self.model[:-1]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight.data, gain=self.gain)\n",
    "                nn.init.zeros_(layer.bias.data)\n",
    "        final_layer = self.model[-1]\n",
    "        nn.init.xavier_normal_(final_layer.weight.data, gain=gain)\n",
    "        nn.init.zeros_(final_layer.bias.data)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print(\"forward\", data.shape)\n",
    "        num_data = data.shape[0]\n",
    "        data = data.view(num_data, -1)\n",
    "        return self.model(data)\n",
    "\n",
    "class MultipleMLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_widths, num_models=1, activation=None,last_layer=None, num_out=1):\n",
    "        nn.Module.__init__(self)\n",
    "        self.models = nn.ModuleList([MLPModel(\n",
    "            input_dim, layer_widths, activation=activation,\n",
    "            last_layer=last_layer, num_out=num_out) for _ in range(num_models)])\n",
    "        self.num_models = num_models\n",
    "\n",
    "    def forward(self, data):\n",
    "        num_data = data.shape[0]\n",
    "        data = data.view(num_data, -1)\n",
    "        outputs = [self.models[i](data) for i in range(self.num_models)]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "def run(exp, args, dataDir, resultDir, train, val, test, device, r):\n",
    "    batch_size = args.regt_batch_size\n",
    "    print('args.regt_lr ',args.regt_lr)\n",
    "    lr = args.regt_lr\n",
    "    num_epoch = args.regt_num_epoch\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "    _logfile = f'{resultDir}/Regression.txt'\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        train.to_tensor()\n",
    "        val.to_tensor()\n",
    "        test.to_tensor()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size)\n",
    "    if args.mode == 'v':\n",
    "        input_dim = args.mV\n",
    "        train_input = train.v\n",
    "        val_input = val.v\n",
    "        test_input = test.v\n",
    "    elif args.mode == 'x':\n",
    "        input_dim = args.mX + args.mXs\n",
    "        train_input = torch.cat((train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.x, test.xs),1)\n",
    "    else:\n",
    "        input_dim = args.mV + args.mX + args.mXs\n",
    "        # print(\"input dim:\", input_dim)\n",
    "        train_input = torch.cat((train.v, train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.v, val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.v, test.x, test.xs),1)\n",
    "\n",
    "    \n",
    "    mlp = MLPModel(input_dim, layer_widths=[128, 64], activation=nn.ReLU(),last_layer=nn.BatchNorm1d(2), num_out=2)\n",
    "    net = nn.Sequential(mlp)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        log(logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\")\n",
    "        log(_logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\", False)\n",
    "        for idx, inputs in enumerate(train_loader):\n",
    "            u = inputs['u']\n",
    "            v = inputs['v']\n",
    "            x = torch.cat((inputs['x'], inputs['xs']), 1)\n",
    "            t = inputs['t'].reshape(-1).type(torch.LongTensor)\n",
    "            # print(\"x:\", x.shape)\n",
    "            # print(\"args.mode:\",args.mode)\n",
    "            if args.mode == 'v':\n",
    "                input_batch = v\n",
    "            elif args.mode == 'x':\n",
    "                input_batch = x\n",
    "                # print(\"input_batch:\", input_batch.shape)\n",
    "            else:\n",
    "                input_batch = torch.cat((v, x),1)\n",
    "            \n",
    "            prediction = net(input_batch) \n",
    "            loss = loss_func(prediction, t)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            loss.backward()        \n",
    "            optimizer.step()    \n",
    "\n",
    "        log(logfile, 'The train accuracy: {:.2f} %'.format((torch.true_divide(sum(train.t.reshape(-1) == torch.max(F.softmax(net(train_input) , dim=1), 1)[1]), len(train.t))).item() * 100))\n",
    "        log(_logfile, 'The test  accuracy: {:.2f} %'.format((torch.true_divide(sum(test.t.reshape(-1) == torch.max(F.softmax(net(test_input) , dim=1), 1)[1]), len(test.t))).item() * 100))\n",
    "\n",
    "    train.s = F.softmax(net(train_input) , dim=1)[:,1:2]\n",
    "    val.s = F.softmax(net(val_input) , dim=1)[:,1:2]\n",
    "    test.s = F.softmax(net(test_input) , dim=1)[:,1:2]\n",
    "    ''' bias rate 1'''\n",
    "    br = [-3.0, -2.5, -2.0, -1.5, -1.3, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "    brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "\n",
    "    return train,val,test\n",
    "\n",
    "\n",
    "def run_ood_IRM(exp, args, dataDir, resultDir, train, val, test, ood_test_dict=None):\n",
    "    batch_size = args.regt_batch_size\n",
    "    lr = args.regt_lr\n",
    "    num_epoch = args.regt_num_epoch\n",
    "    len_loader = 0\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "    _logfile = f'{resultDir}/Regression.txt'\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        train.to_tensor()\n",
    "        val.to_tensor()\n",
    "        test.to_tensor()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if args.mode == 'v':\n",
    "        input_dim = args.mV\n",
    "        train_input = train.v\n",
    "        val_input = val.v\n",
    "        test_input = test.v\n",
    "    elif args.mode == 'x':\n",
    "        input_dim = args.mX + args.mXs\n",
    "        train_input = torch.cat((train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.x, test.xs),1)\n",
    "    else:\n",
    "        input_dim = args.mV + args.mX + args.mXs\n",
    "        # print(\"input dim:\", input_dim)\n",
    "        train_input = torch.cat((train.v, train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.v, val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.v, test.x, test.xs),1)\n",
    "\n",
    "    for r in args.env_list:\n",
    "        train_temp = args.data_dict[r]['train']\n",
    "        val_temp = args.data_dict[r]['val']\n",
    "        test_temp = args.data_dict[r]['test']\n",
    "\n",
    "        try:\n",
    "            train_temp.to_tensor()\n",
    "            val_temp.to_tensor()\n",
    "            test_temp.to_tensor()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        args.data_dict[r]['trainloader_reg'] = DataLoader(train_temp, batch_size=batch_size)\n",
    "        len_loader= len(args.data_dict[r]['trainloader_reg'])\n",
    "\n",
    "    mlp = MLPModel(input_dim, layer_widths=[128, 64], activation=nn.ReLU(),last_layer=nn.BatchNorm1d(2), num_out=2)\n",
    "    net = nn.Sequential(mlp)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    dummy_w = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "    reg = 1e-1\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        log(logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\")\n",
    "        log(_logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\", False)\n",
    "        train_loaders = [iter(args.data_dict[r]['trainloader_reg']) for r in args.env_list]\n",
    "        for _ in range(len_loader):\n",
    "            print(_)\n",
    "            error = 0\n",
    "            penalty = 0\n",
    "            for loader in train_loaders:\n",
    "                inputs = next(loader, None)\n",
    "                if inputs is None:\n",
    "                    print(\"error!\")\n",
    "                u = inputs['u']\n",
    "                v = inputs['v']\n",
    "                x = torch.cat((inputs['x'], inputs['xs']), 1)\n",
    "                t = inputs['t'].reshape(-1).type(torch.LongTensor)\n",
    "                # print(\"x:\", x.shape)\n",
    "                # print(\"args.mode:\",args.mode)\n",
    "                if args.mode == 'v':\n",
    "                    input_batch = v\n",
    "                elif args.mode == 'x':\n",
    "                    input_batch = x\n",
    "                    # print(\"input_batch:\", input_batch.shape)\n",
    "                else:\n",
    "                    input_batch = torch.cat((v, x),1)\n",
    "                \n",
    "                prediction = net(input_batch) \n",
    "                loss = loss_func(prediction * dummy_w, t)\n",
    "                error += loss.mean()\n",
    "                penalty += grad(loss.mean(), dummy_w,\n",
    "                                create_graph=True)[0].pow(2).mean()\n",
    "            optimizer.zero_grad()  \n",
    "            (reg * error + (1 - reg) * penalty).backward()      \n",
    "            optimizer.step()      \n",
    "\n",
    "        log(logfile, 'The train accuracy: {:.2f} %'.format((torch.true_divide(sum(train.t.reshape(-1) == torch.max(F.softmax(net(train_input) , dim=1), 1)[1]), len(train.t))).item() * 100))\n",
    "        log(_logfile, 'The test  accuracy: {:.2f} %'.format((torch.true_divide(sum(test.t.reshape(-1) == torch.max(F.softmax(net(test_input) , dim=1), 1)[1]), len(test.t))).item() * 100))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' bias rate '''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 1.3, 1.5, 2.0, 2.5, 3.0, 0.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "which_benchmark = 'SynOOD2_'+'_'.join(str(item) for item in [args.sc, args.sh, args.one, args.depX, args.depU,args.VX])\n",
    "which_dataset = '_'.join(str(item) for item in [args.mV, args.mX, args.mU, args.mXs])\n",
    "resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}_{args.mode}/ood{brdc[args.ood]}/'\n",
    "dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "logfile = f'{resultDir}/log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data//data/SynOOD2_1_0_1_0.05_0.05_1/2_10_4_2/0/ood_p30/vx/train.csv\n",
      "../../Data//data/SynOOD2_1_0_1_0.05_0.05_1/2_10_4_2/0/ood_n30/vx/test.csv\n",
      "args.regt_lr  0.1\n",
      "Exp 0 :this is the 0/5 epochs.\n",
      "The train accuracy: 84.07 %\n",
      "The test  accuracy: 74.77 %\n",
      "Exp 0 :this is the 1/5 epochs.\n",
      "The train accuracy: 85.76 %\n",
      "The test  accuracy: 75.00 %\n",
      "Exp 0 :this is the 2/5 epochs.\n",
      "The train accuracy: 86.21 %\n",
      "The test  accuracy: 74.84 %\n",
      "Exp 0 :this is the 3/5 epochs.\n",
      "The train accuracy: 86.45 %\n",
      "The test  accuracy: 74.51 %\n",
      "Exp 0 :this is the 4/5 epochs.\n",
      "The train accuracy: 86.61 %\n",
      "The test  accuracy: 74.29 %\n"
     ]
    }
   ],
   "source": [
    "exp = 0\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')\n",
    "train_df2 = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-1.5]}/{args.mode}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/test.csv')\n",
    "# 合并 train 和 val 数据集\n",
    "combined_df = pd.concat([train_df, train_df2], ignore_index=True)\n",
    "# 打乱顺序\n",
    "combined_df = shuffle(combined_df)\n",
    "print(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')\n",
    "print(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/test.csv')\n",
    "# 创建新的数据集\n",
    "combined_dataset = CausalDataset(combined_df, variables=['v', 'u', 'x', 'xs', 'z', 'p', 's', 'm', 't', 'g', 'y', 'f', 'c'])\n",
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "val = CausalDataset(val_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "test = CausalDataset(test_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "train,val,test = run(exp, args, dataDir, resultDir, train, val, test, device, r=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v1', 'v2', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n",
       "       'u1', 'u2', 'u3', 'u4', 'xs1', 'xs2', 'z', 'pi', 't', 'mu0', 'mu1', 'y',\n",
       "       'f'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = 1\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/train.csv')\n",
    "# train = CausalDataset(train_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "train_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.5, 1.5]\n",
      "Exp 0 :this is the 0/5 epochs.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "The train accuracy: 74.91 %\n",
      "The test  accuracy: 72.24 %\n",
      "Exp 0 :this is the 1/5 epochs.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "The train accuracy: 77.31 %\n",
      "The test  accuracy: 75.24 %\n",
      "Exp 0 :this is the 2/5 epochs.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "The train accuracy: 78.96 %\n",
      "The test  accuracy: 77.32 %\n",
      "Exp 0 :this is the 3/5 epochs.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "The train accuracy: 79.84 %\n",
      "The test  accuracy: 78.16 %\n",
      "Exp 0 :this is the 4/5 epochs.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "The train accuracy: 80.56 %\n",
      "The test  accuracy: 79.11 %\n"
     ]
    }
   ],
   "source": [
    "''' OOD test'''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 0.0, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "exp = 0\n",
    "for r in br:\n",
    "    args.data_dict[r] = {\n",
    "        'train': None,\n",
    "        'val': None,\n",
    "        'test': None,\n",
    "        'trainloader_reg': None,\n",
    "        'env': 0,\n",
    "    }\n",
    "    if r in args.env_list:\n",
    "        args.data_dict[r]['env'] = 1\n",
    "        train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/train.csv')\n",
    "        val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/val.csv')\n",
    "        test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/test.csv')\n",
    "\n",
    "        args.data_dict[r]['train'] = CausalDataset(train_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        args.data_dict[r]['val'] = CausalDataset(val_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        args.data_dict[r]['test'] = CausalDataset(test_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "\n",
    "\n",
    "exp = 0\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-1.3]}/{args.mode}/train.csv')\n",
    "train_df2 = pd.read_csv(dataDir + f'{exp}/ood_{brdc[2.5]}/{args.mode}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/val.csv')\n",
    "# 合并 train 和 val 数据集\n",
    "combined_df = pd.concat([train_df, train_df2], ignore_index=True)\n",
    "# 打乱顺序\n",
    "combined_df = shuffle(combined_df)\n",
    "\n",
    "# 创建新的数据集\n",
    "combined_dataset = CausalDataset(combined_df, variables=['v', 'u', 'x', 'xs', 'z', 'p', 's', 'm', 't', 'g', 'y', 'f', 'c'])\n",
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "val = CausalDataset(val_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "test = CausalDataset(test_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "print(args.env_list)\n",
    "run_ood_IRM(exp, args, dataDir, resultDir, train, val, test, ood_test_dict=[]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = args.data_dict[-3.0]['env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.data_dict[-3.0]['env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 0\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 7000\n",
      "测试集大小: 2000\n",
      "验证集大小: 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取数据集\n",
    "data = train_df\n",
    "\n",
    "# 划分训练集和剩余数据\n",
    "train_data, remaining_data = train_test_split(data, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "# 划分测试集和验证集\n",
    "test_data, val_data = train_test_split(remaining_data, test_size=(1/3), shuffle=True, random_state=42)\n",
    "\n",
    "# 打印划分后的数据集大小\n",
    "print(\"训练集大小:\", len(train_data))\n",
    "print(\"测试集大小:\", len(test_data))\n",
    "print(\"验证集大小:\", len(val_data))\n",
    "\n",
    "# 进行后续操作，使用划分后的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>u0</th>\n",
       "      <th>u1</th>\n",
       "      <th>u2</th>\n",
       "      <th>u3</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>...</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>c10</th>\n",
       "      <th>c11</th>\n",
       "      <th>c12</th>\n",
       "      <th>c13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9069</th>\n",
       "      <td>0.524600</td>\n",
       "      <td>-0.525552</td>\n",
       "      <td>0.481662</td>\n",
       "      <td>0.401454</td>\n",
       "      <td>1.898650</td>\n",
       "      <td>1.968174</td>\n",
       "      <td>0.978375</td>\n",
       "      <td>-0.341613</td>\n",
       "      <td>0.629651</td>\n",
       "      <td>0.082054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629651</td>\n",
       "      <td>0.082054</td>\n",
       "      <td>-0.924961</td>\n",
       "      <td>-0.518290</td>\n",
       "      <td>1.281546</td>\n",
       "      <td>0.598323</td>\n",
       "      <td>0.613425</td>\n",
       "      <td>0.756848</td>\n",
       "      <td>0.467906</td>\n",
       "      <td>0.222363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>1.214455</td>\n",
       "      <td>-0.751910</td>\n",
       "      <td>-0.811350</td>\n",
       "      <td>1.006010</td>\n",
       "      <td>-0.449178</td>\n",
       "      <td>0.515667</td>\n",
       "      <td>-0.471500</td>\n",
       "      <td>-0.193685</td>\n",
       "      <td>0.511216</td>\n",
       "      <td>0.235622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511216</td>\n",
       "      <td>0.235622</td>\n",
       "      <td>-0.512427</td>\n",
       "      <td>0.555847</td>\n",
       "      <td>-0.094491</td>\n",
       "      <td>-0.013939</td>\n",
       "      <td>1.032238</td>\n",
       "      <td>0.105693</td>\n",
       "      <td>0.325317</td>\n",
       "      <td>0.432817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7738</th>\n",
       "      <td>-0.671099</td>\n",
       "      <td>0.036636</td>\n",
       "      <td>0.227971</td>\n",
       "      <td>-0.396112</td>\n",
       "      <td>2.877801</td>\n",
       "      <td>0.635873</td>\n",
       "      <td>-0.349373</td>\n",
       "      <td>0.692276</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.229380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.229380</td>\n",
       "      <td>-0.101457</td>\n",
       "      <td>-1.053224</td>\n",
       "      <td>1.369451</td>\n",
       "      <td>0.272344</td>\n",
       "      <td>0.730052</td>\n",
       "      <td>-0.148678</td>\n",
       "      <td>0.866132</td>\n",
       "      <td>1.052830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>-1.863404</td>\n",
       "      <td>-0.307318</td>\n",
       "      <td>1.752581</td>\n",
       "      <td>0.183689</td>\n",
       "      <td>-0.062972</td>\n",
       "      <td>-1.531521</td>\n",
       "      <td>-0.795271</td>\n",
       "      <td>-1.164185</td>\n",
       "      <td>1.105108</td>\n",
       "      <td>-0.432358</td>\n",
       "      <td>...</td>\n",
       "      <td>1.105108</td>\n",
       "      <td>-0.432358</td>\n",
       "      <td>-0.586890</td>\n",
       "      <td>-0.518608</td>\n",
       "      <td>1.229404</td>\n",
       "      <td>0.649537</td>\n",
       "      <td>-0.490955</td>\n",
       "      <td>0.066665</td>\n",
       "      <td>1.139130</td>\n",
       "      <td>0.955422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5058</th>\n",
       "      <td>-0.705511</td>\n",
       "      <td>1.106909</td>\n",
       "      <td>-0.992365</td>\n",
       "      <td>-0.317162</td>\n",
       "      <td>1.583554</td>\n",
       "      <td>0.884582</td>\n",
       "      <td>-0.386704</td>\n",
       "      <td>0.997796</td>\n",
       "      <td>0.503503</td>\n",
       "      <td>-0.681674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503503</td>\n",
       "      <td>-0.681674</td>\n",
       "      <td>0.281742</td>\n",
       "      <td>0.384268</td>\n",
       "      <td>-0.628569</td>\n",
       "      <td>-0.117194</td>\n",
       "      <td>-0.575836</td>\n",
       "      <td>0.612674</td>\n",
       "      <td>0.679975</td>\n",
       "      <td>0.800622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>-0.428441</td>\n",
       "      <td>-1.260926</td>\n",
       "      <td>-1.175668</td>\n",
       "      <td>0.752964</td>\n",
       "      <td>-1.181665</td>\n",
       "      <td>-1.542833</td>\n",
       "      <td>-0.270879</td>\n",
       "      <td>0.104049</td>\n",
       "      <td>0.112153</td>\n",
       "      <td>-0.230140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112153</td>\n",
       "      <td>-0.230140</td>\n",
       "      <td>-1.179994</td>\n",
       "      <td>-0.036267</td>\n",
       "      <td>0.631529</td>\n",
       "      <td>0.602401</td>\n",
       "      <td>1.270601</td>\n",
       "      <td>-0.883364</td>\n",
       "      <td>0.836381</td>\n",
       "      <td>0.779818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>-0.221534</td>\n",
       "      <td>-0.403937</td>\n",
       "      <td>-0.472669</td>\n",
       "      <td>1.350004</td>\n",
       "      <td>-0.472227</td>\n",
       "      <td>0.822671</td>\n",
       "      <td>-1.263000</td>\n",
       "      <td>0.322743</td>\n",
       "      <td>-0.788642</td>\n",
       "      <td>-1.244502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788642</td>\n",
       "      <td>-1.244502</td>\n",
       "      <td>-1.398260</td>\n",
       "      <td>-0.039018</td>\n",
       "      <td>-1.279876</td>\n",
       "      <td>0.152198</td>\n",
       "      <td>-0.146648</td>\n",
       "      <td>-1.779244</td>\n",
       "      <td>1.645399</td>\n",
       "      <td>1.345456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>0.902519</td>\n",
       "      <td>0.426075</td>\n",
       "      <td>0.611076</td>\n",
       "      <td>1.684176</td>\n",
       "      <td>1.037696</td>\n",
       "      <td>-0.433580</td>\n",
       "      <td>-0.121180</td>\n",
       "      <td>0.517547</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>-0.191259</td>\n",
       "      <td>0.664572</td>\n",
       "      <td>-0.807385</td>\n",
       "      <td>0.818987</td>\n",
       "      <td>0.415126</td>\n",
       "      <td>0.225441</td>\n",
       "      <td>-0.618124</td>\n",
       "      <td>0.206968</td>\n",
       "      <td>0.544344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1.659318</td>\n",
       "      <td>0.078774</td>\n",
       "      <td>0.930513</td>\n",
       "      <td>-0.303925</td>\n",
       "      <td>0.326301</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>-0.043024</td>\n",
       "      <td>-1.330140</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>-0.455293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>-0.455293</td>\n",
       "      <td>0.229716</td>\n",
       "      <td>0.383941</td>\n",
       "      <td>-0.910247</td>\n",
       "      <td>0.768532</td>\n",
       "      <td>0.665644</td>\n",
       "      <td>1.180827</td>\n",
       "      <td>0.082144</td>\n",
       "      <td>0.333329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>1.887944</td>\n",
       "      <td>0.356941</td>\n",
       "      <td>2.148641</td>\n",
       "      <td>-0.291734</td>\n",
       "      <td>0.709542</td>\n",
       "      <td>1.057250</td>\n",
       "      <td>0.648191</td>\n",
       "      <td>0.379612</td>\n",
       "      <td>0.070827</td>\n",
       "      <td>0.437266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070827</td>\n",
       "      <td>0.437266</td>\n",
       "      <td>0.609723</td>\n",
       "      <td>-0.477012</td>\n",
       "      <td>1.206010</td>\n",
       "      <td>0.502513</td>\n",
       "      <td>-0.866156</td>\n",
       "      <td>0.487778</td>\n",
       "      <td>-0.135604</td>\n",
       "      <td>0.207302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            v0        v1        u0        u1        u2        u3        x0  \\\n",
       "9069  0.524600 -0.525552  0.481662  0.401454  1.898650  1.968174  0.978375   \n",
       "2603  1.214455 -0.751910 -0.811350  1.006010 -0.449178  0.515667 -0.471500   \n",
       "7738 -0.671099  0.036636  0.227971 -0.396112  2.877801  0.635873 -0.349373   \n",
       "1579 -1.863404 -0.307318  1.752581  0.183689 -0.062972 -1.531521 -0.795271   \n",
       "5058 -0.705511  1.106909 -0.992365 -0.317162  1.583554  0.884582 -0.386704   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5734 -0.428441 -1.260926 -1.175668  0.752964 -1.181665 -1.542833 -0.270879   \n",
       "5191 -0.221534 -0.403937 -0.472669  1.350004 -0.472227  0.822671 -1.263000   \n",
       "5390  0.902519  0.426075  0.611076  1.684176  1.037696 -0.433580 -0.121180   \n",
       "860   1.659318  0.078774  0.930513 -0.303925  0.326301  0.635371 -0.043024   \n",
       "7270  1.887944  0.356941  2.148641 -0.291734  0.709542  1.057250  0.648191   \n",
       "\n",
       "            x1        x2        x3  ...        c4        c5        c6  \\\n",
       "9069 -0.341613  0.629651  0.082054  ...  0.629651  0.082054 -0.924961   \n",
       "2603 -0.193685  0.511216  0.235622  ...  0.511216  0.235622 -0.512427   \n",
       "7738  0.692276 -0.002813  0.229380  ... -0.002813  0.229380 -0.101457   \n",
       "1579 -1.164185  1.105108 -0.432358  ...  1.105108 -0.432358 -0.586890   \n",
       "5058  0.997796  0.503503 -0.681674  ...  0.503503 -0.681674  0.281742   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5734  0.104049  0.112153 -0.230140  ...  0.112153 -0.230140 -1.179994   \n",
       "5191  0.322743 -0.788642 -1.244502  ... -0.788642 -1.244502 -1.398260   \n",
       "5390  0.517547  0.117253 -0.191259  ...  0.117253 -0.191259  0.664572   \n",
       "860  -1.330140  0.505785 -0.455293  ...  0.505785 -0.455293  0.229716   \n",
       "7270  0.379612  0.070827  0.437266  ...  0.070827  0.437266  0.609723   \n",
       "\n",
       "            c7        c8        c9       c10       c11       c12       c13  \n",
       "9069 -0.518290  1.281546  0.598323  0.613425  0.756848  0.467906  0.222363  \n",
       "2603  0.555847 -0.094491 -0.013939  1.032238  0.105693  0.325317  0.432817  \n",
       "7738 -1.053224  1.369451  0.272344  0.730052 -0.148678  0.866132  1.052830  \n",
       "1579 -0.518608  1.229404  0.649537 -0.490955  0.066665  1.139130  0.955422  \n",
       "5058  0.384268 -0.628569 -0.117194 -0.575836  0.612674  0.679975  0.800622  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5734 -0.036267  0.631529  0.602401  1.270601 -0.883364  0.836381  0.779818  \n",
       "5191 -0.039018 -1.279876  0.152198 -0.146648 -1.779244  1.645399  1.345456  \n",
       "5390 -0.807385  0.818987  0.415126  0.225441 -0.618124  0.206968  0.544344  \n",
       "860   0.383941 -0.910247  0.768532  0.665644  1.180827  0.082144  0.333329  \n",
       "7270 -0.477012  1.206010  0.502513 -0.866156  0.487778 -0.135604  0.207302  \n",
       "\n",
       "[7000 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='../Data/Causal/'\n",
    "ihdp_train_path = dataDir + 'ihdp_npci_1-1000.test.npz'\n",
    "ihdp_train = np.load(ihdp_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 25, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihdp_train['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'xs1', 'xs2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['x' + str(i+1) for i in range(10)] + ['xs' + str(i+1) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
