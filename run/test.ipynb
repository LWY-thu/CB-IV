{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch.autograd import grad\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append(r\"../\")\n",
    "sys.path.append(r\"../../\")\n",
    "sys.path.append('/home/wyliu/code/CB-IV')\n",
    "from utils import log, CausalDataset\n",
    "from module.SynCBIV import run as run_SynCBIV\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def get_args():\n",
    "    argparser = argparse.ArgumentParser(description=__doc__)\n",
    "    # About run setting !!!!\n",
    "    argparser.add_argument('--seed',default=2021,type=int,help='The random seed')\n",
    "    argparser.add_argument('--mode',default='vx',type=str,help='The choice of v/x/vx/xx')\n",
    "    argparser.add_argument('--ood',default=-3.0,type=float,help='The train dataset of OOD')\n",
    "    argparser.add_argument('--ood_test',default=3.0,type=float,help='The train dataset of OOD')\n",
    "    argparser.add_argument('--rewrite_log',default=False,type=bool,help='Whether rewrite log file')\n",
    "    argparser.add_argument('--use_gpu',default=1,type=int,help='The use of GPU')\n",
    "    argparser.add_argument('--des_str',default='/_/',type=str,help='The description of this running')\n",
    "    argparser.add_argument('--oodtestall',default=0,type=int,help='The random seed')\n",
    "    argparser.add_argument('--iter',default=3000,type=int,help='The num of iterations')\n",
    "    # About data setting ~~~~\n",
    "    argparser.add_argument('--num',default=10000,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--num_reps',default=100,type=int,help='The num of train\\val\\test dataset')\n",
    "    argparser.add_argument('--ate',default=0,type=float,help='The ate of constant')\n",
    "    argparser.add_argument('--sc',default=1,type=float,help='The sc')\n",
    "    argparser.add_argument('--sh',default=0,type=float,help='The sh')\n",
    "    argparser.add_argument('--one',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--depX',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--depU',default=0.05,type=float,help='Whether generates harder datasets')\n",
    "    argparser.add_argument('--VX',default=1,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mV',default=2,type=int,help='The dim of Instrumental variables V')\n",
    "    argparser.add_argument('--mX',default=10,type=int,help='The dim of Confounding variables X')\n",
    "    argparser.add_argument('--mU',default=4,type=int,help='The dim of Unobserved confounding variables U')\n",
    "    argparser.add_argument('--mXs',default=2,type=int,help='The dim of Noise variables X')\n",
    "    argparser.add_argument('--storage_path',default='../../Data/',type=str,help='The dir of data storage')\n",
    "    # Syn\n",
    "    argparser.add_argument('--syn_alpha',default=0.01,type=float,help='')\n",
    "    argparser.add_argument('--syn_lambda',default=0.001,type=float,help='')\n",
    "    argparser.add_argument('--syn_twoStage',default=True,type=bool,help='')\n",
    "    argparser.add_argument('--lrate',default=0.001,type=float,help='learning rate')\n",
    "    # About Debug or Show\n",
    "    argparser.add_argument('--verbose',default=1,type=int,help='The level of verbose')\n",
    "    argparser.add_argument('--epoch_show',default=5,type=int,help='The epochs of show time')\n",
    "    # About Regression_t\n",
    "    argparser.add_argument('--regt_batch_size',default=500,type=int,help='The size of one batch')\n",
    "    argparser.add_argument('--regt_lr',default=0.1,type=float,help='The learning rate')\n",
    "    argparser.add_argument('--regt_num_epoch',default=10,type=int,help='The num of total epoch')\n",
    "    # About IRM  \n",
    "    argparser.add_argument('--env_list',default=[-3.0, -1.5],type=list,help='The environment list')\n",
    "    argparser.add_argument('--data_dict',default={},type=dict,help='The data dict')\n",
    "    # args = argparser.parse_args()\n",
    "    args = argparser.parse_args(args=[])\n",
    "    return args\n",
    "\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import set_seed, log\n",
    "\n",
    "def get_gain(activation):\n",
    "    if activation.__class__.__name__ == \"LeakyReLU\":\n",
    "        gain = nn.init.calculate_gain(\"leaky_relu\",\n",
    "                                            activation.negative_slope)\n",
    "    else:\n",
    "        activation_name = activation.__class__.__name__.lower()\n",
    "        try:\n",
    "            gain = nn.init.calculate_gain(activation_name)\n",
    "        except ValueError:\n",
    "            gain = 1.0\n",
    "    return gain\n",
    "\n",
    "# input_dim：输入数据的维度。\n",
    "# layer_widths：一个整数列表，表示隐藏层的宽度。\n",
    "# activation：激活函数（默认为 None）。\n",
    "# last_layer：可选的最后一层，可以是任何 nn.Module 的子类（默认为 None）。\n",
    "# num_out：输出的维度（默认为 1）。\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_widths, activation=None,last_layer=None, num_out=1):\n",
    "        nn.Module.__init__(self)\n",
    "        self.gain=get_gain(activation)\n",
    "        # 根据隐藏层的宽度列表 layer_widths，\n",
    "        # 创建一系列的线性层（nn.Linear），\n",
    "        # 并可选择地在每个线性层之后添加给定的激活函数 activation。\n",
    "        # 最后，根据输出维度 num_out 添加最后一层线性层。\n",
    "        if len(layer_widths) == 0:\n",
    "            layers = [nn.Linear(input_dim, num_out)]\n",
    "        else:\n",
    "            num_layers = len(layer_widths)\n",
    "            if activation is None:\n",
    "                layers = [nn.Linear(input_dim, layer_widths[0])]\n",
    "            else:\n",
    "                layers = [nn.Linear(input_dim, layer_widths[0]), activation]\n",
    "            for i in range(1, num_layers):\n",
    "                w_in = layer_widths[i-1]\n",
    "                w_out = layer_widths[i]\n",
    "                if activation is None:\n",
    "                    layers.extend([nn.Linear(w_in, w_out)])\n",
    "                else:\n",
    "                    layers.extend([nn.Linear(w_in, w_out), activation])\n",
    "            layers.append(nn.Linear(layer_widths[-1], num_out))\n",
    "        if last_layer:\n",
    "            layers.append(last_layer)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def initialize(self, gain=1.0):\n",
    "        # initialize 方法用于初始化模型的参数。\n",
    "        for layer in self.model[:-1]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight.data, gain=self.gain)\n",
    "                nn.init.zeros_(layer.bias.data)\n",
    "        final_layer = self.model[-1]\n",
    "        nn.init.xavier_normal_(final_layer.weight.data, gain=gain)\n",
    "        nn.init.zeros_(final_layer.bias.data)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print(\"forward\", data.shape)\n",
    "        num_data = data.shape[0]\n",
    "        data = data.view(num_data, -1)\n",
    "        return self.model(data)\n",
    "\n",
    "class MultipleMLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_widths, num_models=1, activation=None,last_layer=None, num_out=1):\n",
    "        nn.Module.__init__(self)\n",
    "        self.models = nn.ModuleList([MLPModel(\n",
    "            input_dim, layer_widths, activation=activation,\n",
    "            last_layer=last_layer, num_out=num_out) for _ in range(num_models)])\n",
    "        self.num_models = num_models\n",
    "\n",
    "    def forward(self, data):\n",
    "        num_data = data.shape[0]\n",
    "        data = data.view(num_data, -1)\n",
    "        outputs = [self.models[i](data) for i in range(self.num_models)]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "def run(exp, args, dataDir, resultDir, train, val, test, device, r):\n",
    "    batch_size = args.regt_batch_size\n",
    "    lr = args.regt_lr\n",
    "    num_epoch = args.regt_num_epoch\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "    _logfile = f'{resultDir}/Regression.txt'\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        train.to_tensor()\n",
    "        val.to_tensor()\n",
    "        test.to_tensor()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size)\n",
    "    if args.mode == 'v':\n",
    "        input_dim = args.mV\n",
    "        train_input = train.v\n",
    "        val_input = val.v\n",
    "        test_input = test.v\n",
    "    elif args.mode == 'x':\n",
    "        input_dim = args.mX + args.mXs\n",
    "        train_input = torch.cat((train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.x, test.xs),1)\n",
    "    else:\n",
    "        input_dim = args.mV + args.mX + args.mXs\n",
    "        # print(\"input dim:\", input_dim)\n",
    "        train_input = torch.cat((train.v, train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.v, val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.v, test.x, test.xs),1)\n",
    "\n",
    "    \n",
    "    mlp = MLPModel(input_dim, layer_widths=[128, 64], activation=nn.ReLU(),last_layer=nn.BatchNorm1d(2), num_out=2)\n",
    "    net = nn.Sequential(mlp)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        log(logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\")\n",
    "        log(_logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\", False)\n",
    "        for idx, inputs in enumerate(train_loader):\n",
    "            u = inputs['u']\n",
    "            v = inputs['v']\n",
    "            x = torch.cat((inputs['x'], inputs['xs']), 1)\n",
    "            t = inputs['t'].reshape(-1).type(torch.LongTensor)\n",
    "            # print(\"x:\", x.shape)\n",
    "            # print(\"args.mode:\",args.mode)\n",
    "            if args.mode == 'v':\n",
    "                input_batch = v\n",
    "            elif args.mode == 'x':\n",
    "                input_batch = x\n",
    "                # print(\"input_batch:\", input_batch.shape)\n",
    "            else:\n",
    "                input_batch = torch.cat((v, x),1)\n",
    "            \n",
    "            prediction = net(input_batch) \n",
    "            loss = loss_func(prediction, t)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            loss.backward()        \n",
    "            optimizer.step()    \n",
    "\n",
    "        log(logfile, 'The train accuracy: {:.2f} %'.format((torch.true_divide(sum(train.t.reshape(-1) == torch.max(F.softmax(net(train_input) , dim=1), 1)[1]), len(train.t))).item() * 100))\n",
    "        log(_logfile, 'The test  accuracy: {:.2f} %'.format((torch.true_divide(sum(test.t.reshape(-1) == torch.max(F.softmax(net(test_input) , dim=1), 1)[1]), len(test.t))).item() * 100))\n",
    "\n",
    "    train.s = F.softmax(net(train_input) , dim=1)[:,1:2]\n",
    "    val.s = F.softmax(net(val_input) , dim=1)[:,1:2]\n",
    "    test.s = F.softmax(net(test_input) , dim=1)[:,1:2]\n",
    "    ''' bias rate 1'''\n",
    "    br = [-3.0, -2.5, -2.0, -1.5, -1.3, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "    brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "\n",
    "    return train,val,test\n",
    "\n",
    "\n",
    "def run_ood_IRM(exp, args, dataDir, resultDir, train, val, test, ood_test_dict=None):\n",
    "    batch_size = args.regt_batch_size\n",
    "    lr = args.regt_lr\n",
    "    num_epoch = args.regt_num_epoch\n",
    "    len_loader = 0\n",
    "    logfile = f'{resultDir}/log.txt'\n",
    "    _logfile = f'{resultDir}/Regression.txt'\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        train.to_tensor()\n",
    "        val.to_tensor()\n",
    "        test.to_tensor()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if args.mode == 'v':\n",
    "        input_dim = args.mV\n",
    "        train_input = train.v\n",
    "        val_input = val.v\n",
    "        test_input = test.v\n",
    "    elif args.mode == 'x':\n",
    "        input_dim = args.mX + args.mXs\n",
    "        train_input = torch.cat((train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.x, test.xs),1)\n",
    "    else:\n",
    "        input_dim = args.mV + args.mX + args.mXs\n",
    "        # print(\"input dim:\", input_dim)\n",
    "        train_input = torch.cat((train.v, train.x, train.xs),1)\n",
    "        val_input = torch.cat((val.v, val.x, val.xs),1)\n",
    "        test_input = torch.cat((test.v, test.x, test.xs),1)\n",
    "\n",
    "    for r in args.env_list:\n",
    "        train_temp = args.data_dict[r]['train']\n",
    "        val_temp = args.data_dict[r]['val']\n",
    "        test_temp = args.data_dict[r]['test']\n",
    "\n",
    "        try:\n",
    "            train_temp.to_tensor()\n",
    "            val_temp.to_tensor()\n",
    "            test_temp.to_tensor()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        args.data_dict[r]['trainloader_reg'] = DataLoader(train_temp, batch_size=batch_size)\n",
    "        len_loader= len(args.data_dict[r]['trainloader_reg'])\n",
    "\n",
    "    mlp = MLPModel(input_dim, layer_widths=[128, 64], activation=nn.ReLU(),last_layer=nn.BatchNorm1d(2), num_out=2)\n",
    "    net = nn.Sequential(mlp)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    dummy_w = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "    reg = 1e-1\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        log(logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\")\n",
    "        log(_logfile, f\"Exp {exp} :this is the {epoch}/{num_epoch} epochs.\", False)\n",
    "        train_loaders = [iter(args.data_dict[r]['trainloader_reg']) for r in args.env_list]\n",
    "        for _ in range(len_loader):\n",
    "            # print(_)\n",
    "            error = 0\n",
    "            penalty = 0\n",
    "            for loader in train_loaders:\n",
    "                inputs = next(loader, None)\n",
    "                if inputs is None:\n",
    "                    print(\"error!\")\n",
    "                u = inputs['u']\n",
    "                v = inputs['v']\n",
    "                x = torch.cat((inputs['x'], inputs['xs']), 1)\n",
    "                t = inputs['t'].reshape(-1).type(torch.LongTensor)\n",
    "                # print(\"x:\", x.shape)\n",
    "                # print(\"args.mode:\",args.mode)\n",
    "                if args.mode == 'v':\n",
    "                    input_batch = v\n",
    "                elif args.mode == 'x':\n",
    "                    input_batch = x\n",
    "                    # print(\"input_batch:\", input_batch.shape)\n",
    "                else:\n",
    "                    input_batch = torch.cat((v, x),1)\n",
    "                \n",
    "                prediction = net(input_batch) \n",
    "                loss = loss_func(prediction * dummy_w, t)\n",
    "                error += loss.mean()\n",
    "                penalty += grad(loss.mean(), dummy_w,\n",
    "                                create_graph=True)[0].pow(2).mean()\n",
    "            optimizer.zero_grad()  \n",
    "            (reg * error + (1 - reg) * penalty).backward()      \n",
    "            optimizer.step()      \n",
    "\n",
    "        log(logfile, 'The train accuracy: {:.2f} %'.format((torch.true_divide(sum(train.t.reshape(-1) == torch.max(F.softmax(net(train_input) , dim=1), 1)[1]), len(train.t))).item() * 100))\n",
    "        log(_logfile, 'The test  accuracy: {:.2f} %'.format((torch.true_divide(sum(test.t.reshape(-1) == torch.max(F.softmax(net(test_input) , dim=1), 1)[1]), len(test.t))).item() * 100))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' bias rate '''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 1.3, 1.5, 2.0, 2.5, 3.0, 0.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "which_benchmark = 'SynOOD2_'+'_'.join(str(item) for item in [args.sc, args.sh, args.one, args.depX, args.depU,args.VX])\n",
    "which_dataset = '_'.join(str(item) for item in [args.mV, args.mX, args.mU, args.mXs])\n",
    "resultDir = args.storage_path + f'/results/{which_benchmark}_{which_dataset}_{args.mode}/ood{brdc[args.ood]}/'\n",
    "dataDir = f'{args.storage_path}/data/{which_benchmark}/{which_dataset}/'\n",
    "os.makedirs(os.path.dirname(resultDir), exist_ok=True)\n",
    "logfile = f'{resultDir}/log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data//data/SynOOD2_1_0_1_0.05_0.05_1/2_10_4_2/0/ood_p30/vx/train.csv\n",
      "../../Data//data/SynOOD2_1_0_1_0.05_0.05_1/2_10_4_2/0/ood_n30/vx/test.csv\n",
      "Exp 0 :this is the 0/10 epochs.\n",
      "The train accuracy: 84.07 %\n",
      "The test  accuracy: 74.77 %\n",
      "Exp 0 :this is the 1/10 epochs.\n",
      "The train accuracy: 85.76 %\n",
      "The test  accuracy: 75.00 %\n",
      "Exp 0 :this is the 2/10 epochs.\n",
      "The train accuracy: 86.21 %\n",
      "The test  accuracy: 74.84 %\n",
      "Exp 0 :this is the 3/10 epochs.\n",
      "The train accuracy: 86.45 %\n",
      "The test  accuracy: 74.51 %\n",
      "Exp 0 :this is the 4/10 epochs.\n",
      "The train accuracy: 86.61 %\n",
      "The test  accuracy: 74.29 %\n",
      "Exp 0 :this is the 5/10 epochs.\n",
      "The train accuracy: 86.73 %\n",
      "The test  accuracy: 73.84 %\n",
      "Exp 0 :this is the 6/10 epochs.\n",
      "The train accuracy: 86.97 %\n",
      "The test  accuracy: 73.71 %\n",
      "Exp 0 :this is the 7/10 epochs.\n",
      "The train accuracy: 87.13 %\n",
      "The test  accuracy: 73.43 %\n",
      "Exp 0 :this is the 8/10 epochs.\n",
      "The train accuracy: 87.27 %\n",
      "The test  accuracy: 73.17 %\n",
      "Exp 0 :this is the 9/10 epochs.\n",
      "The train accuracy: 87.40 %\n",
      "The test  accuracy: 73.00 %\n"
     ]
    }
   ],
   "source": [
    "exp = 0\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')\n",
    "train_df2 = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-1.5]}/{args.mode}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/test.csv')\n",
    "# 合并 train 和 val 数据集\n",
    "combined_df = pd.concat([train_df, train_df2], ignore_index=True)\n",
    "# 打乱顺序\n",
    "combined_df = shuffle(combined_df)\n",
    "print(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/train.csv')\n",
    "print(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/test.csv')\n",
    "# 创建新的数据集\n",
    "combined_dataset = CausalDataset(combined_df, variables=['v', 'u', 'x', 'xs', 'z', 'p', 's', 'm', 't', 'g', 'y', 'f', 'c'])\n",
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "val = CausalDataset(val_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "test = CausalDataset(test_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "train,val,test = run(exp, args, dataDir, resultDir, train, val, test, device, r=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.loadDataset.CausalDataset at 0x7faac42c1f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.5]\n",
      "Exp 0 :this is the 0/10 epochs.\n",
      "The train accuracy: 75.09 %\n",
      "The test  accuracy: 70.71 %\n",
      "Exp 0 :this is the 1/10 epochs.\n",
      "The train accuracy: 77.01 %\n",
      "The test  accuracy: 72.20 %\n",
      "Exp 0 :this is the 2/10 epochs.\n",
      "The train accuracy: 78.23 %\n",
      "The test  accuracy: 73.68 %\n",
      "Exp 0 :this is the 3/10 epochs.\n",
      "The train accuracy: 79.21 %\n",
      "The test  accuracy: 74.34 %\n",
      "Exp 0 :this is the 4/10 epochs.\n",
      "The train accuracy: 79.82 %\n",
      "The test  accuracy: 74.91 %\n",
      "Exp 0 :this is the 5/10 epochs.\n",
      "The train accuracy: 80.43 %\n",
      "The test  accuracy: 75.14 %\n",
      "Exp 0 :this is the 6/10 epochs.\n",
      "The train accuracy: 80.83 %\n",
      "The test  accuracy: 75.41 %\n",
      "Exp 0 :this is the 7/10 epochs.\n",
      "The train accuracy: 81.11 %\n",
      "The test  accuracy: 75.93 %\n",
      "Exp 0 :this is the 8/10 epochs.\n",
      "The train accuracy: 81.30 %\n",
      "The test  accuracy: 76.27 %\n",
      "Exp 0 :this is the 9/10 epochs.\n",
      "The train accuracy: 81.47 %\n",
      "The test  accuracy: 76.52 %\n"
     ]
    }
   ],
   "source": [
    "''' OOD test'''\n",
    "br = [-3.0, -2.5, -2.0, -1.5, -1.3, 0.0, 1.3, 1.5, 2.0, 2.5, 3.0]\n",
    "brdc = {-3.0: 'n30', -2.5:'n25', -2.0:'n20', -1.5:'n15', -1.3:'n13', 1.3:'p13', 1.5:'p15', 2.0:'p20', 2.5:'p25', 3.0:'p30', 0.0:'0'}\n",
    "exp = 0\n",
    "for r in br:\n",
    "    args.data_dict[r] = {\n",
    "        'train': None,\n",
    "        'val': None,\n",
    "        'test': None,\n",
    "        'trainloader_reg': None,\n",
    "        'env': 0,\n",
    "    }\n",
    "    if r in args.env_list:\n",
    "        args.data_dict[r]['env'] = 1\n",
    "        train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/train.csv')\n",
    "        val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/val.csv')\n",
    "        test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/test.csv')\n",
    "\n",
    "        args.data_dict[r]['train'] = CausalDataset(train_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        args.data_dict[r]['val'] = CausalDataset(val_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "        args.data_dict[r]['test'] = CausalDataset(test_df, variables = ['u','x','v','xs','z','p','s','m','t','g','y','f','c'], observe_vars=['v','x','xs'])\n",
    "\n",
    "\n",
    "exp = 0\n",
    "if args.use_gpu:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-1.3]}/{args.mode}/train.csv')\n",
    "train_df2 = pd.read_csv(dataDir + f'{exp}/ood_{brdc[2.5]}/{args.mode}/train.csv')\n",
    "val_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[-3.0]}/{args.mode}/val.csv')\n",
    "test_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[3.0]}/{args.mode}/val.csv')\n",
    "# 合并 train 和 val 数据集\n",
    "combined_df = pd.concat([train_df, train_df2], ignore_index=True)\n",
    "# 打乱顺序\n",
    "combined_df = shuffle(combined_df)\n",
    "\n",
    "# 创建新的数据集\n",
    "combined_dataset = CausalDataset(combined_df, variables=['v', 'u', 'x', 'xs', 'z', 'p', 's', 'm', 't', 'g', 'y', 'f', 'c'])\n",
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "val = CausalDataset(val_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "test = CausalDataset(test_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])\n",
    "print(args.env_list)\n",
    "run_ood_IRM(exp, args, dataDir, resultDir, train, val, test, ood_test_dict=[]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(dataDir + f'{exp}/ood_{brdc[r]}/{args.mode}/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-728b4ccbc479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/wyliu/yes/envs/tf-torch/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "train = CausalDataset(train_df, variables = ['v','u','x','xs','z','p','s','m','t','g','y','f','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
